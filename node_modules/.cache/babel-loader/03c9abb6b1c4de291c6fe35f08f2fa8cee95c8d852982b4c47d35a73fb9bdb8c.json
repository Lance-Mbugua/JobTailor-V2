{"ast":null,"code":"// src/openai-compatible-chat-language-model.ts\nimport { InvalidResponseDataError } from \"@ai-sdk/provider\";\nimport { combineHeaders, createEventSourceResponseHandler, createJsonErrorResponseHandler, createJsonResponseHandler, generateId, isParsableJson, postJsonToApi } from \"@ai-sdk/provider-utils\";\nimport { z as z2 } from \"zod\";\n\n// src/convert-to-openai-compatible-chat-messages.ts\nimport { UnsupportedFunctionalityError } from \"@ai-sdk/provider\";\nimport { convertUint8ArrayToBase64 } from \"@ai-sdk/provider-utils\";\nfunction getOpenAIMetadata(message) {\n  var _a, _b;\n  return (_b = (_a = message == null ? void 0 : message.providerMetadata) == null ? void 0 : _a.openaiCompatible) != null ? _b : {};\n}\nfunction convertToOpenAICompatibleChatMessages(prompt) {\n  const messages = [];\n  for (const {\n    role,\n    content,\n    ...message\n  } of prompt) {\n    const metadata = getOpenAIMetadata({\n      ...message\n    });\n    switch (role) {\n      case \"system\":\n        {\n          messages.push({\n            role: \"system\",\n            content,\n            ...metadata\n          });\n          break;\n        }\n      case \"user\":\n        {\n          if (content.length === 1 && content[0].type === \"text\") {\n            messages.push({\n              role: \"user\",\n              content: content[0].text,\n              ...getOpenAIMetadata(content[0])\n            });\n            break;\n          }\n          messages.push({\n            role: \"user\",\n            content: content.map(part => {\n              var _a;\n              const partMetadata = getOpenAIMetadata(part);\n              switch (part.type) {\n                case \"text\":\n                  {\n                    return {\n                      type: \"text\",\n                      text: part.text,\n                      ...partMetadata\n                    };\n                  }\n                case \"image\":\n                  {\n                    return {\n                      type: \"image_url\",\n                      image_url: {\n                        url: part.image instanceof URL ? part.image.toString() : `data:${(_a = part.mimeType) != null ? _a : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`\n                      },\n                      ...partMetadata\n                    };\n                  }\n                case \"file\":\n                  {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: \"File content parts in user messages\"\n                    });\n                  }\n              }\n            }),\n            ...metadata\n          });\n          break;\n        }\n      case \"assistant\":\n        {\n          let text = \"\";\n          const toolCalls = [];\n          for (const part of content) {\n            const partMetadata = getOpenAIMetadata(part);\n            switch (part.type) {\n              case \"text\":\n                {\n                  text += part.text;\n                  break;\n                }\n              case \"tool-call\":\n                {\n                  toolCalls.push({\n                    id: part.toolCallId,\n                    type: \"function\",\n                    function: {\n                      name: part.toolName,\n                      arguments: JSON.stringify(part.args)\n                    },\n                    ...partMetadata\n                  });\n                  break;\n                }\n            }\n          }\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            tool_calls: toolCalls.length > 0 ? toolCalls : void 0,\n            ...metadata\n          });\n          break;\n        }\n      case \"tool\":\n        {\n          for (const toolResponse of content) {\n            const toolResponseMetadata = getOpenAIMetadata(toolResponse);\n            messages.push({\n              role: \"tool\",\n              tool_call_id: toolResponse.toolCallId,\n              content: JSON.stringify(toolResponse.result),\n              ...toolResponseMetadata\n            });\n          }\n          break;\n        }\n      default:\n        {\n          const _exhaustiveCheck = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  return messages;\n}\n\n// src/get-response-metadata.ts\nfunction getResponseMetadata({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\n\n// src/map-openai-compatible-finish-reason.ts\nfunction mapOpenAICompatibleFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\n\n// src/openai-compatible-error.ts\nimport { z } from \"zod\";\nvar openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish()\n  })\n});\nvar defaultOpenAICompatibleErrorStructure = {\n  errorSchema: openaiCompatibleErrorDataSchema,\n  errorToMessage: data => data.error.message\n};\n\n// src/openai-compatible-prepare-tools.ts\nimport { UnsupportedFunctionalityError as UnsupportedFunctionalityError2 } from \"@ai-sdk/provider\";\nfunction prepareTools({\n  mode,\n  structuredOutputs\n}) {\n  var _a;\n  const tools = ((_a = mode.tools) == null ? void 0 : _a.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return {\n      tools: void 0,\n      tool_choice: void 0,\n      toolWarnings\n    };\n  }\n  const toolChoice = mode.toolChoice;\n  const openaiCompatTools = [];\n  for (const tool of tools) {\n    if (tool.type === \"provider-defined\") {\n      toolWarnings.push({\n        type: \"unsupported-tool\",\n        tool\n      });\n    } else {\n      openaiCompatTools.push({\n        type: \"function\",\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return {\n      tools: openaiCompatTools,\n      tool_choice: void 0,\n      toolWarnings\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return {\n        tools: openaiCompatTools,\n        tool_choice: type,\n        toolWarnings\n      };\n    case \"tool\":\n      return {\n        tools: openaiCompatTools,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default:\n      {\n        const _exhaustiveCheck = type;\n        throw new UnsupportedFunctionalityError2({\n          functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n        });\n      }\n  }\n}\n\n// src/openai-compatible-chat-language-model.ts\nvar OpenAICompatibleChatLanguageModel = class {\n  // type inferred via constructor\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    var _a, _b;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    const errorStructure = (_a = config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(errorStructure.errorSchema);\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n    this.supportsStructuredOutputs = (_b = config.supportsStructuredOutputs) != null ? _b : false;\n  }\n  get defaultObjectGenerationMode() {\n    return this.config.defaultObjectGenerationMode;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerMetadata,\n    stopSequences,\n    responseFormat,\n    seed\n  }) {\n    var _a, _b;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !this.supportsStructuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? this.supportsStructuredOutputs === true && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          name: (_a = responseFormat.name) != null ? _a : \"response\",\n          description: responseFormat.description\n        }\n      } : {\n        type: \"json_object\"\n      } : void 0,\n      stop: stopSequences,\n      seed,\n      ...(providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]),\n      // messages:\n      messages: convertToOpenAICompatibleChatMessages(prompt)\n    };\n    switch (type) {\n      case \"regular\":\n        {\n          const {\n            tools,\n            tool_choice,\n            toolWarnings\n          } = prepareTools({\n            mode,\n            structuredOutputs: this.supportsStructuredOutputs\n          });\n          return {\n            args: {\n              ...baseArgs,\n              tools,\n              tool_choice\n            },\n            warnings: [...warnings, ...toolWarnings]\n          };\n        }\n      case \"object-json\":\n        {\n          return {\n            args: {\n              ...baseArgs,\n              response_format: this.supportsStructuredOutputs === true && mode.schema != null ? {\n                type: \"json_schema\",\n                json_schema: {\n                  schema: mode.schema,\n                  name: (_b = mode.name) != null ? _b : \"response\",\n                  description: mode.description\n                }\n              } : {\n                type: \"json_object\"\n              }\n            },\n            warnings\n          };\n        }\n      case \"object-tool\":\n        {\n          return {\n            args: {\n              ...baseArgs,\n              tool_choice: {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name\n                }\n              },\n              tools: [{\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters\n                }\n              }]\n            },\n            warnings\n          };\n        }\n      default:\n        {\n          const _exhaustiveCheck = type;\n          throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d, _e, _f, _g, _h, _i;\n    const {\n      args,\n      warnings\n    } = this.getArgs({\n      ...options\n    });\n    const body = JSON.stringify(args);\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(OpenAICompatibleChatResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      messages: rawPrompt,\n      ...rawSettings\n    } = args;\n    const choice = responseBody.choices[0];\n    const providerMetadata = (_b = (_a = this.config.metadataExtractor) == null ? void 0 : _a.extractMetadata) == null ? void 0 : _b.call(_a, {\n      parsedBody: rawResponse\n    });\n    return {\n      text: (_c = choice.message.content) != null ? _c : void 0,\n      reasoning: (_d = choice.message.reasoning_content) != null ? _d : void 0,\n      toolCalls: (_e = choice.message.tool_calls) == null ? void 0 : _e.map(toolCall => {\n        var _a2;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a2 = toolCall.id) != null ? _a2 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_g = (_f = responseBody.usage) == null ? void 0 : _f.prompt_tokens) != null ? _g : NaN,\n        completionTokens: (_i = (_h = responseBody.usage) == null ? void 0 : _h.completion_tokens) != null ? _i : NaN\n      },\n      ...(providerMetadata && {\n        providerMetadata\n      }),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      response: getResponseMetadata(responseBody),\n      warnings,\n      request: {\n        body\n      }\n    };\n  }\n  async doStream(options) {\n    var _a;\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({\n            type: \"response-metadata\",\n            ...result.response\n          });\n          if (result.reasoning) {\n            if (Array.isArray(result.reasoning)) {\n              for (const part of result.reasoning) {\n                if (part.type === \"text\") {\n                  controller.enqueue({\n                    type: \"reasoning\",\n                    textDelta: part.text\n                  });\n                }\n              }\n            } else {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: result.reasoning\n              });\n            }\n          }\n          if (result.text) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: result.text\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: \"tool-call\",\n                ...toolCall\n              });\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings\n      };\n    }\n    const {\n      args,\n      warnings\n    } = this.getArgs({\n      ...options\n    });\n    const body = JSON.stringify({\n      ...args,\n      stream: true\n    });\n    const metadataExtractor = (_a = this.config.metadataExtractor) == null ? void 0 : _a.createStreamExtractor();\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: {\n        ...args,\n        stream: true\n      },\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(this.chunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      messages: rawPrompt,\n      ...rawSettings\n    } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: void 0,\n      completionTokens: void 0\n    };\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(new TransformStream({\n        // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n        transform(chunk, controller) {\n          var _a2, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m, _n;\n          if (!chunk.success) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: chunk.error\n            });\n            return;\n          }\n          const value = chunk.value;\n          metadataExtractor == null ? void 0 : metadataExtractor.processChunk(chunk.rawValue);\n          if (\"error\" in value) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: value.error.message\n            });\n            return;\n          }\n          if (isFirstChunk) {\n            isFirstChunk = false;\n            controller.enqueue({\n              type: \"response-metadata\",\n              ...getResponseMetadata(value)\n            });\n          }\n          if (value.usage != null) {\n            usage = {\n              promptTokens: (_a2 = value.usage.prompt_tokens) != null ? _a2 : void 0,\n              completionTokens: (_b = value.usage.completion_tokens) != null ? _b : void 0\n            };\n          }\n          const choice = value.choices[0];\n          if ((choice == null ? void 0 : choice.finish_reason) != null) {\n            finishReason = mapOpenAICompatibleFinishReason(choice.finish_reason);\n          }\n          if ((choice == null ? void 0 : choice.delta) == null) {\n            return;\n          }\n          const delta = choice.delta;\n          if (delta.reasoning_content != null) {\n            controller.enqueue({\n              type: \"reasoning\",\n              textDelta: delta.reasoning_content\n            });\n          }\n          if (delta.content != null) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: delta.content\n            });\n          }\n          if (delta.tool_calls != null) {\n            for (const toolCallDelta of delta.tool_calls) {\n              const index = toolCallDelta.index;\n              if (toolCalls[index] == null) {\n                if (toolCallDelta.type !== \"function\") {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'function' type.`\n                  });\n                }\n                if (toolCallDelta.id == null) {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'id' to be a string.`\n                  });\n                }\n                if (((_c = toolCallDelta.function) == null ? void 0 : _c.name) == null) {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'function.name' to be a string.`\n                  });\n                }\n                toolCalls[index] = {\n                  id: toolCallDelta.id,\n                  type: \"function\",\n                  function: {\n                    name: toolCallDelta.function.name,\n                    arguments: (_d = toolCallDelta.function.arguments) != null ? _d : \"\"\n                  },\n                  hasFinished: false\n                };\n                const toolCall2 = toolCalls[index];\n                if (((_e = toolCall2.function) == null ? void 0 : _e.name) != null && ((_f = toolCall2.function) == null ? void 0 : _f.arguments) != null) {\n                  if (toolCall2.function.arguments.length > 0) {\n                    controller.enqueue({\n                      type: \"tool-call-delta\",\n                      toolCallType: \"function\",\n                      toolCallId: toolCall2.id,\n                      toolName: toolCall2.function.name,\n                      argsTextDelta: toolCall2.function.arguments\n                    });\n                  }\n                  if (isParsableJson(toolCall2.function.arguments)) {\n                    controller.enqueue({\n                      type: \"tool-call\",\n                      toolCallType: \"function\",\n                      toolCallId: (_g = toolCall2.id) != null ? _g : generateId(),\n                      toolName: toolCall2.function.name,\n                      args: toolCall2.function.arguments\n                    });\n                    toolCall2.hasFinished = true;\n                  }\n                }\n                continue;\n              }\n              const toolCall = toolCalls[index];\n              if (toolCall.hasFinished) {\n                continue;\n              }\n              if (((_h = toolCallDelta.function) == null ? void 0 : _h.arguments) != null) {\n                toolCall.function.arguments += (_j = (_i = toolCallDelta.function) == null ? void 0 : _i.arguments) != null ? _j : \"\";\n              }\n              controller.enqueue({\n                type: \"tool-call-delta\",\n                toolCallType: \"function\",\n                toolCallId: toolCall.id,\n                toolName: toolCall.function.name,\n                argsTextDelta: (_k = toolCallDelta.function.arguments) != null ? _k : \"\"\n              });\n              if (((_l = toolCall.function) == null ? void 0 : _l.name) != null && ((_m = toolCall.function) == null ? void 0 : _m.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallType: \"function\",\n                  toolCallId: (_n = toolCall.id) != null ? _n : generateId(),\n                  toolName: toolCall.function.name,\n                  args: toolCall.function.arguments\n                });\n                toolCall.hasFinished = true;\n              }\n            }\n          }\n        },\n        flush(controller) {\n          var _a2, _b;\n          const metadata = metadataExtractor == null ? void 0 : metadataExtractor.buildMetadata();\n          controller.enqueue({\n            type: \"finish\",\n            finishReason,\n            usage: {\n              promptTokens: (_a2 = usage.promptTokens) != null ? _a2 : NaN,\n              completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n            },\n            ...(metadata && {\n              providerMetadata: metadata\n            })\n          });\n        }\n      })),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders\n      },\n      warnings,\n      request: {\n        body\n      }\n    };\n  }\n};\nvar OpenAICompatibleChatResponseSchema = z2.object({\n  id: z2.string().nullish(),\n  created: z2.number().nullish(),\n  model: z2.string().nullish(),\n  choices: z2.array(z2.object({\n    message: z2.object({\n      role: z2.literal(\"assistant\").nullish(),\n      content: z2.string().nullish(),\n      reasoning_content: z2.string().nullish(),\n      tool_calls: z2.array(z2.object({\n        id: z2.string().nullish(),\n        type: z2.literal(\"function\"),\n        function: z2.object({\n          name: z2.string(),\n          arguments: z2.string()\n        })\n      })).nullish()\n    }),\n    finish_reason: z2.string().nullish()\n  })),\n  usage: z2.object({\n    prompt_tokens: z2.number().nullish(),\n    completion_tokens: z2.number().nullish()\n  }).nullish()\n});\nvar createOpenAICompatibleChatChunkSchema = errorSchema => z2.union([z2.object({\n  id: z2.string().nullish(),\n  created: z2.number().nullish(),\n  model: z2.string().nullish(),\n  choices: z2.array(z2.object({\n    delta: z2.object({\n      role: z2.enum([\"assistant\"]).nullish(),\n      content: z2.string().nullish(),\n      reasoning_content: z2.string().nullish(),\n      tool_calls: z2.array(z2.object({\n        index: z2.number(),\n        id: z2.string().nullish(),\n        type: z2.literal(\"function\").optional(),\n        function: z2.object({\n          name: z2.string().nullish(),\n          arguments: z2.string().nullish()\n        })\n      })).nullish()\n    }).nullish(),\n    finish_reason: z2.string().nullish()\n  })),\n  usage: z2.object({\n    prompt_tokens: z2.number().nullish(),\n    completion_tokens: z2.number().nullish()\n  }).nullish()\n}), errorSchema]);\n\n// src/openai-compatible-completion-language-model.ts\nimport { UnsupportedFunctionalityError as UnsupportedFunctionalityError4 } from \"@ai-sdk/provider\";\nimport { combineHeaders as combineHeaders2, createEventSourceResponseHandler as createEventSourceResponseHandler2, createJsonErrorResponseHandler as createJsonErrorResponseHandler2, createJsonResponseHandler as createJsonResponseHandler2, postJsonToApi as postJsonToApi2 } from \"@ai-sdk/provider-utils\";\nimport { z as z3 } from \"zod\";\n\n// src/convert-to-openai-compatible-completion-prompt.ts\nimport { InvalidPromptError, UnsupportedFunctionalityError as UnsupportedFunctionalityError3 } from \"@ai-sdk/provider\";\nfunction convertToOpenAICompatibleCompletionPrompt({\n  prompt,\n  inputFormat,\n  user = \"user\",\n  assistant = \"assistant\"\n}) {\n  if (inputFormat === \"prompt\" && prompt.length === 1 && prompt[0].role === \"user\" && prompt[0].content.length === 1 && prompt[0].content[0].type === \"text\") {\n    return {\n      prompt: prompt[0].content[0].text\n    };\n  }\n  let text = \"\";\n  if (prompt[0].role === \"system\") {\n    text += `${prompt[0].content}\n\n`;\n    prompt = prompt.slice(1);\n  }\n  for (const {\n    role,\n    content\n  } of prompt) {\n    switch (role) {\n      case \"system\":\n        {\n          throw new InvalidPromptError({\n            message: \"Unexpected system message in prompt: ${content}\",\n            prompt\n          });\n        }\n      case \"user\":\n        {\n          const userMessage = content.map(part => {\n            switch (part.type) {\n              case \"text\":\n                {\n                  return part.text;\n                }\n              case \"image\":\n                {\n                  throw new UnsupportedFunctionalityError3({\n                    functionality: \"images\"\n                  });\n                }\n            }\n          }).join(\"\");\n          text += `${user}:\n${userMessage}\n\n`;\n          break;\n        }\n      case \"assistant\":\n        {\n          const assistantMessage = content.map(part => {\n            switch (part.type) {\n              case \"text\":\n                {\n                  return part.text;\n                }\n              case \"tool-call\":\n                {\n                  throw new UnsupportedFunctionalityError3({\n                    functionality: \"tool-call messages\"\n                  });\n                }\n            }\n          }).join(\"\");\n          text += `${assistant}:\n${assistantMessage}\n\n`;\n          break;\n        }\n      case \"tool\":\n        {\n          throw new UnsupportedFunctionalityError3({\n            functionality: \"tool messages\"\n          });\n        }\n      default:\n        {\n          const _exhaustiveCheck = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  text += `${assistant}:\n`;\n  return {\n    prompt: text,\n    stopSequences: [`\n${user}:`]\n  };\n}\n\n// src/openai-compatible-completion-language-model.ts\nvar OpenAICompatibleCompletionLanguageModel = class {\n  // type inferred via constructor\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = void 0;\n    var _a;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    const errorStructure = (_a = config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleCompletionChunkSchema(errorStructure.errorSchema);\n    this.failedResponseHandler = createJsonErrorResponseHandler2(errorStructure);\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n    providerMetadata\n  }) {\n    var _a;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const {\n      prompt: completionPrompt,\n      stopSequences\n    } = convertToOpenAICompatibleCompletionPrompt({\n      prompt,\n      inputFormat\n    });\n    const stop = [...(stopSequences != null ? stopSequences : []), ...(userStopSequences != null ? userStopSequences : [])];\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      ...(providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]),\n      // prompt:\n      prompt: completionPrompt,\n      // stop sequences:\n      stop: stop.length > 0 ? stop : void 0\n    };\n    switch (type) {\n      case \"regular\":\n        {\n          if ((_a = mode.tools) == null ? void 0 : _a.length) {\n            throw new UnsupportedFunctionalityError4({\n              functionality: \"tools\"\n            });\n          }\n          if (mode.toolChoice) {\n            throw new UnsupportedFunctionalityError4({\n              functionality: \"toolChoice\"\n            });\n          }\n          return {\n            args: baseArgs,\n            warnings\n          };\n        }\n      case \"object-json\":\n        {\n          throw new UnsupportedFunctionalityError4({\n            functionality: \"object-json mode\"\n          });\n        }\n      case \"object-tool\":\n        {\n          throw new UnsupportedFunctionalityError4({\n            functionality: \"object-tool mode\"\n          });\n        }\n      default:\n        {\n          const _exhaustiveCheck = type;\n          throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d;\n    const {\n      args,\n      warnings\n    } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler2(openaiCompatibleCompletionResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      prompt: rawPrompt,\n      ...rawSettings\n    } = args;\n    const choice = response.choices[0];\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: (_b = (_a = response.usage) == null ? void 0 : _a.prompt_tokens) != null ? _b : NaN,\n        completionTokens: (_d = (_c = response.usage) == null ? void 0 : _c.completion_tokens) != null ? _d : NaN\n      },\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      response: getResponseMetadata(response),\n      warnings,\n      request: {\n        body: JSON.stringify(args)\n      }\n    };\n  }\n  async doStream(options) {\n    const {\n      args,\n      warnings\n    } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true\n    };\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler2(this.chunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      prompt: rawPrompt,\n      ...rawSettings\n    } = args;\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(new TransformStream({\n        transform(chunk, controller) {\n          if (!chunk.success) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: chunk.error\n            });\n            return;\n          }\n          const value = chunk.value;\n          if (\"error\" in value) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: value.error\n            });\n            return;\n          }\n          if (isFirstChunk) {\n            isFirstChunk = false;\n            controller.enqueue({\n              type: \"response-metadata\",\n              ...getResponseMetadata(value)\n            });\n          }\n          if (value.usage != null) {\n            usage = {\n              promptTokens: value.usage.prompt_tokens,\n              completionTokens: value.usage.completion_tokens\n            };\n          }\n          const choice = value.choices[0];\n          if ((choice == null ? void 0 : choice.finish_reason) != null) {\n            finishReason = mapOpenAICompatibleFinishReason(choice.finish_reason);\n          }\n          if ((choice == null ? void 0 : choice.text) != null) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: choice.text\n            });\n          }\n        },\n        flush(controller) {\n          controller.enqueue({\n            type: \"finish\",\n            finishReason,\n            usage\n          });\n        }\n      })),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders\n      },\n      warnings,\n      request: {\n        body: JSON.stringify(body)\n      }\n    };\n  }\n};\nvar openaiCompatibleCompletionResponseSchema = z3.object({\n  id: z3.string().nullish(),\n  created: z3.number().nullish(),\n  model: z3.string().nullish(),\n  choices: z3.array(z3.object({\n    text: z3.string(),\n    finish_reason: z3.string()\n  })),\n  usage: z3.object({\n    prompt_tokens: z3.number(),\n    completion_tokens: z3.number()\n  }).nullish()\n});\nvar createOpenAICompatibleCompletionChunkSchema = errorSchema => z3.union([z3.object({\n  id: z3.string().nullish(),\n  created: z3.number().nullish(),\n  model: z3.string().nullish(),\n  choices: z3.array(z3.object({\n    text: z3.string(),\n    finish_reason: z3.string().nullish(),\n    index: z3.number()\n  })),\n  usage: z3.object({\n    prompt_tokens: z3.number(),\n    completion_tokens: z3.number()\n  }).nullish()\n}), errorSchema]);\n\n// src/openai-compatible-embedding-model.ts\nimport { TooManyEmbeddingValuesForCallError } from \"@ai-sdk/provider\";\nimport { combineHeaders as combineHeaders3, createJsonErrorResponseHandler as createJsonErrorResponseHandler3, createJsonResponseHandler as createJsonResponseHandler3, postJsonToApi as postJsonToApi3 } from \"@ai-sdk/provider-utils\";\nimport { z as z4 } from \"zod\";\nvar OpenAICompatibleEmbeddingModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get maxEmbeddingsPerCall() {\n    var _a;\n    return (_a = this.config.maxEmbeddingsPerCall) != null ? _a : 2048;\n  }\n  get supportsParallelCalls() {\n    var _a;\n    return (_a = this.config.supportsParallelCalls) != null ? _a : true;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal\n  }) {\n    var _a;\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi3({\n      url: this.config.url({\n        path: \"/embeddings\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders3(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: \"float\",\n        dimensions: this.settings.dimensions,\n        user: this.settings.user\n      },\n      failedResponseHandler: createJsonErrorResponseHandler3((_a = this.config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure),\n      successfulResponseHandler: createJsonResponseHandler3(openaiTextEmbeddingResponseSchema),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage ? {\n        tokens: response.usage.prompt_tokens\n      } : void 0,\n      rawResponse: {\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiTextEmbeddingResponseSchema = z4.object({\n  data: z4.array(z4.object({\n    embedding: z4.array(z4.number())\n  })),\n  usage: z4.object({\n    prompt_tokens: z4.number()\n  }).nullish()\n});\n\n// src/openai-compatible-image-model.ts\nimport { combineHeaders as combineHeaders4, createJsonErrorResponseHandler as createJsonErrorResponseHandler4, createJsonResponseHandler as createJsonResponseHandler4, postJsonToApi as postJsonToApi4 } from \"@ai-sdk/provider-utils\";\nimport { z as z5 } from \"zod\";\nvar OpenAICompatibleImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get maxImagesPerCall() {\n    var _a;\n    return (_a = this.settings.maxImagesPerCall) != null ? _a : 10;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a, _b, _c, _d, _e;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    const currentDate = (_c = (_b = (_a = this.config._internal) == null ? void 0 : _a.currentDate) == null ? void 0 : _b.call(_a)) != null ? _c : /* @__PURE__ */new Date();\n    const {\n      value: response,\n      responseHeaders\n    } = await postJsonToApi4({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders4(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...((_d = providerOptions.openai) != null ? _d : {}),\n        response_format: \"b64_json\",\n        ...(this.settings.user ? {\n          user: this.settings.user\n        } : {})\n      },\n      failedResponseHandler: createJsonErrorResponseHandler4((_e = this.config.errorStructure) != null ? _e : defaultOpenAICompatibleErrorStructure),\n      successfulResponseHandler: createJsonResponseHandler4(openaiCompatibleImageResponseSchema),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiCompatibleImageResponseSchema = z5.object({\n  data: z5.array(z5.object({\n    b64_json: z5.string()\n  }))\n});\n\n// src/openai-compatible-provider.ts\nimport { withoutTrailingSlash } from \"@ai-sdk/provider-utils\";\nfunction createOpenAICompatible(options) {\n  const baseURL = withoutTrailingSlash(options.baseURL);\n  const providerName = options.name;\n  const getHeaders = () => ({\n    ...(options.apiKey && {\n      Authorization: `Bearer ${options.apiKey}`\n    }),\n    ...options.headers\n  });\n  const getCommonModelConfig = modelType => ({\n    provider: `${providerName}.${modelType}`,\n    url: ({\n      path\n    }) => {\n      const url = new URL(`${baseURL}${path}`);\n      if (options.queryParams) {\n        url.search = new URLSearchParams(options.queryParams).toString();\n      }\n      return url.toString();\n    },\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId, settings = {}) => createChatModel(modelId, settings);\n  const createChatModel = (modelId, settings = {}) => new OpenAICompatibleChatLanguageModel(modelId, settings, {\n    ...getCommonModelConfig(\"chat\"),\n    defaultObjectGenerationMode: \"tool\"\n  });\n  const createCompletionModel = (modelId, settings = {}) => new OpenAICompatibleCompletionLanguageModel(modelId, settings, getCommonModelConfig(\"completion\"));\n  const createEmbeddingModel = (modelId, settings = {}) => new OpenAICompatibleEmbeddingModel(modelId, settings, getCommonModelConfig(\"embedding\"));\n  const createImageModel = (modelId, settings = {}) => new OpenAICompatibleImageModel(modelId, settings, getCommonModelConfig(\"image\"));\n  const provider = (modelId, settings) => createLanguageModel(modelId, settings);\n  provider.languageModel = createLanguageModel;\n  provider.chatModel = createChatModel;\n  provider.completionModel = createCompletionModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.imageModel = createImageModel;\n  return provider;\n}\nexport { OpenAICompatibleChatLanguageModel, OpenAICompatibleCompletionLanguageModel, OpenAICompatibleEmbeddingModel, OpenAICompatibleImageModel, createOpenAICompatible };","map":{"version":3,"names":["InvalidResponseDataError","combineHeaders","createEventSourceResponseHandler","createJsonErrorResponseHandler","createJsonResponseHandler","generateId","isParsableJson","postJsonToApi","z","z2","UnsupportedFunctionalityError","convertUint8ArrayToBase64","getOpenAIMetadata","message","_a","_b","providerMetadata","openaiCompatible","convertToOpenAICompatibleChatMessages","prompt","messages","role","content","metadata","push","length","type","text","map","part","partMetadata","image_url","url","image","URL","toString","mimeType","functionality","toolCalls","id","toolCallId","function","name","toolName","arguments","JSON","stringify","args","tool_calls","toolResponse","toolResponseMetadata","tool_call_id","result","_exhaustiveCheck","Error","getResponseMetadata","model","created","modelId","timestamp","Date","mapOpenAICompatibleFinishReason","finishReason","openaiCompatibleErrorDataSchema","object","error","string","nullish","param","any","code","union","number","defaultOpenAICompatibleErrorStructure","errorSchema","errorToMessage","data","UnsupportedFunctionalityError2","prepareTools","mode","structuredOutputs","tools","toolWarnings","tool_choice","toolChoice","openaiCompatTools","tool","description","parameters","OpenAICompatibleChatLanguageModel","constructor","settings","config","specificationVersion","errorStructure","chunkSchema","createOpenAICompatibleChatChunkSchema","failedResponseHandler","supportsStructuredOutputs","defaultObjectGenerationMode","provider","providerOptionsName","split","trim","getArgs","maxTokens","temperature","topP","topK","frequencyPenalty","presencePenalty","stopSequences","responseFormat","seed","warnings","setting","schema","details","baseArgs","user","max_tokens","top_p","frequency_penalty","presence_penalty","response_format","json_schema","stop","doGenerate","options","_c","_d","_e","_f","_g","_h","_i","body","responseHeaders","value","responseBody","rawValue","rawResponse","path","headers","successfulResponseHandler","OpenAICompatibleChatResponseSchema","abortSignal","fetch","rawPrompt","rawSettings","choice","choices","metadataExtractor","extractMetadata","call","parsedBody","reasoning","reasoning_content","toolCall","_a2","toolCallType","finish_reason","usage","promptTokens","prompt_tokens","NaN","completionTokens","completion_tokens","rawCall","response","request","doStream","simulateStreaming","simulatedStream","ReadableStream","start","controller","enqueue","Array","isArray","textDelta","logprobs","close","stream","createStreamExtractor","isFirstChunk","pipeThrough","TransformStream","transform","chunk","_j","_k","_l","_m","_n","success","processChunk","delta","toolCallDelta","index","hasFinished","toolCall2","argsTextDelta","flush","buildMetadata","array","literal","enum","optional","UnsupportedFunctionalityError4","combineHeaders2","createEventSourceResponseHandler2","createJsonErrorResponseHandler2","createJsonResponseHandler2","postJsonToApi2","z3","InvalidPromptError","UnsupportedFunctionalityError3","convertToOpenAICompatibleCompletionPrompt","inputFormat","assistant","slice","userMessage","join","assistantMessage","OpenAICompatibleCompletionLanguageModel","createOpenAICompatibleCompletionChunkSchema","userStopSequences","completionPrompt","echo","logit_bias","logitBias","suffix","openaiCompatibleCompletionResponseSchema","Number","TooManyEmbeddingValuesForCallError","combineHeaders3","createJsonErrorResponseHandler3","createJsonResponseHandler3","postJsonToApi3","z4","OpenAICompatibleEmbeddingModel","maxEmbeddingsPerCall","supportsParallelCalls","doEmbed","values","input","encoding_format","dimensions","openaiTextEmbeddingResponseSchema","embeddings","item","embedding","tokens","combineHeaders4","createJsonErrorResponseHandler4","createJsonResponseHandler4","postJsonToApi4","z5","OpenAICompatibleImageModel","maxImagesPerCall","n","size","aspectRatio","providerOptions","currentDate","_internal","openai","openaiCompatibleImageResponseSchema","images","b64_json","withoutTrailingSlash","createOpenAICompatible","baseURL","providerName","getHeaders","apiKey","Authorization","getCommonModelConfig","modelType","queryParams","search","URLSearchParams","createLanguageModel","createChatModel","createCompletionModel","createEmbeddingModel","createImageModel","languageModel","chatModel","completionModel","textEmbeddingModel","imageModel"],"sources":["C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-chat-language-model.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\convert-to-openai-compatible-chat-messages.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\get-response-metadata.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\map-openai-compatible-finish-reason.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-error.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-prepare-tools.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-completion-language-model.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\convert-to-openai-compatible-completion-prompt.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-embedding-model.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-image-model.ts","C:\\Users\\owner\\JobTailor V2\\frontend\\node_modules\\@ai-sdk\\openai-compatible\\src\\openai-compatible-provider.ts"],"sourcesContent":["import {\n  APICallError,\n  InvalidResponseDataError,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1ObjectGenerationMode,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  generateId,\n  isParsableJson,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAICompatibleChatMessages } from './convert-to-openai-compatible-chat-messages';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleChatModelId,\n  OpenAICompatibleChatSettings,\n} from './openai-compatible-chat-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\nimport { prepareTools } from './openai-compatible-prepare-tools';\nimport { MetadataExtractor } from './openai-compatible-metadata-extractor';\n\nexport type OpenAICompatibleChatConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n  metadataExtractor?: MetadataExtractor;\n\n  /**\nDefault object generation mode that should be used with this model when\nno mode is specified. Should be the mode with the best results for this\nmodel. `undefined` can be specified if object generation is not supported.\n  */\n  defaultObjectGenerationMode?: LanguageModelV1ObjectGenerationMode;\n\n  /**\n   * Whether the model supports structured outputs.\n   */\n  supportsStructuredOutputs?: boolean;\n};\n\nexport class OpenAICompatibleChatLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  readonly supportsStructuredOutputs: boolean;\n\n  readonly modelId: OpenAICompatibleChatModelId;\n  readonly settings: OpenAICompatibleChatSettings;\n\n  private readonly config: OpenAICompatibleChatConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleChatModelId,\n    settings: OpenAICompatibleChatSettings,\n    config: OpenAICompatibleChatConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n\n    this.supportsStructuredOutputs = config.supportsStructuredOutputs ?? false;\n  }\n\n  get defaultObjectGenerationMode(): 'json' | 'tool' | undefined {\n    return this.config.defaultObjectGenerationMode;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  private getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerMetadata,\n    stopSequences,\n    responseFormat,\n    seed,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (\n      responseFormat?.type === 'json' &&\n      responseFormat.schema != null &&\n      !this.supportsStructuredOutputs\n    ) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details:\n          'JSON response format schema is only supported with structuredOutputs',\n      });\n    }\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      user: this.settings.user,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format:\n        responseFormat?.type === 'json'\n          ? this.supportsStructuredOutputs === true &&\n            responseFormat.schema != null\n            ? {\n                type: 'json_schema',\n                json_schema: {\n                  schema: responseFormat.schema,\n                  name: responseFormat.name ?? 'response',\n                  description: responseFormat.description,\n                },\n              }\n            : { type: 'json_object' }\n          : undefined,\n\n      stop: stopSequences,\n      seed,\n      ...providerMetadata?.[this.providerOptionsName],\n\n      // messages:\n      messages: convertToOpenAICompatibleChatMessages(prompt),\n    };\n\n    switch (type) {\n      case 'regular': {\n        const { tools, tool_choice, toolWarnings } = prepareTools({\n          mode,\n          structuredOutputs: this.supportsStructuredOutputs,\n        });\n\n        return {\n          args: { ...baseArgs, tools, tool_choice },\n          warnings: [...warnings, ...toolWarnings],\n        };\n      }\n\n      case 'object-json': {\n        return {\n          args: {\n            ...baseArgs,\n            response_format:\n              this.supportsStructuredOutputs === true && mode.schema != null\n                ? {\n                    type: 'json_schema',\n                    json_schema: {\n                      schema: mode.schema,\n                      name: mode.name ?? 'response',\n                      description: mode.description,\n                    },\n                  }\n                : { type: 'json_object' },\n          },\n          warnings,\n        };\n      }\n\n      case 'object-tool': {\n        return {\n          args: {\n            ...baseArgs,\n            tool_choice: {\n              type: 'function',\n              function: { name: mode.tool.name },\n            },\n            tools: [\n              {\n                type: 'function',\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters,\n                },\n              },\n            ],\n          },\n          warnings,\n        };\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args, warnings } = this.getArgs({ ...options });\n\n    const body = JSON.stringify(args);\n\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        OpenAICompatibleChatResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = args;\n    const choice = responseBody.choices[0];\n    const providerMetadata = this.config.metadataExtractor?.extractMetadata?.({\n      parsedBody: rawResponse,\n    });\n\n    return {\n      text: choice.message.content ?? undefined,\n      reasoning: choice.message.reasoning_content ?? undefined,\n      toolCalls: choice.message.tool_calls?.map(toolCall => ({\n        toolCallType: 'function',\n        toolCallId: toolCall.id ?? generateId(),\n        toolName: toolCall.function.name,\n        args: toolCall.function.arguments!,\n      })),\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: responseBody.usage?.prompt_tokens ?? NaN,\n        completionTokens: responseBody.usage?.completion_tokens ?? NaN,\n      },\n      ...(providerMetadata && { providerMetadata }),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(responseBody),\n      warnings,\n      request: { body },\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream<LanguageModelV1StreamPart>({\n        start(controller) {\n          controller.enqueue({ type: 'response-metadata', ...result.response });\n          if (result.reasoning) {\n            if (Array.isArray(result.reasoning)) {\n              for (const part of result.reasoning) {\n                if (part.type === 'text') {\n                  controller.enqueue({\n                    type: 'reasoning',\n                    textDelta: part.text,\n                  });\n                }\n              }\n            } else {\n              controller.enqueue({\n                type: 'reasoning',\n                textDelta: result.reasoning,\n              });\n            }\n          }\n          if (result.text) {\n            controller.enqueue({\n              type: 'text-delta',\n              textDelta: result.text,\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: 'tool-call',\n                ...toolCall,\n              });\n            }\n          }\n          controller.enqueue({\n            type: 'finish',\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata,\n          });\n          controller.close();\n        },\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings,\n      };\n    }\n\n    const { args, warnings } = this.getArgs({ ...options });\n\n    const body = JSON.stringify({ ...args, stream: true });\n    const metadataExtractor =\n      this.config.metadataExtractor?.createStreamExtractor();\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/chat/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: {\n        ...args,\n        stream: true,\n      },\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { messages: rawPrompt, ...rawSettings } = args;\n\n    const toolCalls: Array<{\n      id: string;\n      type: 'function';\n      function: {\n        name: string;\n        arguments: string;\n      };\n      hasFinished: boolean;\n    }> = [];\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: {\n      promptTokens: number | undefined;\n      completionTokens: number | undefined;\n    } = {\n      promptTokens: undefined,\n      completionTokens: undefined,\n    };\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n            const value = chunk.value;\n\n            metadataExtractor?.processChunk(chunk.rawValue);\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error.message });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens ?? undefined,\n                completionTokens: value.usage.completion_tokens ?? undefined,\n              };\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.delta == null) {\n              return;\n            }\n\n            const delta = choice.delta;\n\n            // enqueue reasoning before text deltas:\n            if (delta.reasoning_content != null) {\n              controller.enqueue({\n                type: 'reasoning',\n                textDelta: delta.reasoning_content,\n              });\n            }\n\n            if (delta.content != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: delta.content,\n              });\n            }\n\n            if (delta.tool_calls != null) {\n              for (const toolCallDelta of delta.tool_calls) {\n                const index = toolCallDelta.index;\n\n                if (toolCalls[index] == null) {\n                  if (toolCallDelta.type !== 'function') {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function' type.`,\n                    });\n                  }\n\n                  if (toolCallDelta.id == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'id' to be a string.`,\n                    });\n                  }\n\n                  if (toolCallDelta.function?.name == null) {\n                    throw new InvalidResponseDataError({\n                      data: toolCallDelta,\n                      message: `Expected 'function.name' to be a string.`,\n                    });\n                  }\n\n                  toolCalls[index] = {\n                    id: toolCallDelta.id,\n                    type: 'function',\n                    function: {\n                      name: toolCallDelta.function.name,\n                      arguments: toolCallDelta.function.arguments ?? '',\n                    },\n                    hasFinished: false,\n                  };\n\n                  const toolCall = toolCalls[index];\n\n                  if (\n                    toolCall.function?.name != null &&\n                    toolCall.function?.arguments != null\n                  ) {\n                    // send delta if the argument text has already started:\n                    if (toolCall.function.arguments.length > 0) {\n                      controller.enqueue({\n                        type: 'tool-call-delta',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id,\n                        toolName: toolCall.function.name,\n                        argsTextDelta: toolCall.function.arguments,\n                      });\n                    }\n\n                    // check if tool call is complete\n                    // (some providers send the full tool call in one chunk):\n                    if (isParsableJson(toolCall.function.arguments)) {\n                      controller.enqueue({\n                        type: 'tool-call',\n                        toolCallType: 'function',\n                        toolCallId: toolCall.id ?? generateId(),\n                        toolName: toolCall.function.name,\n                        args: toolCall.function.arguments,\n                      });\n                      toolCall.hasFinished = true;\n                    }\n                  }\n\n                  continue;\n                }\n\n                // existing tool call, merge if not finished\n                const toolCall = toolCalls[index];\n\n                if (toolCall.hasFinished) {\n                  continue;\n                }\n\n                if (toolCallDelta.function?.arguments != null) {\n                  toolCall.function!.arguments +=\n                    toolCallDelta.function?.arguments ?? '';\n                }\n\n                // send delta\n                controller.enqueue({\n                  type: 'tool-call-delta',\n                  toolCallType: 'function',\n                  toolCallId: toolCall.id,\n                  toolName: toolCall.function.name,\n                  argsTextDelta: toolCallDelta.function.arguments ?? '',\n                });\n\n                // check if tool call is complete\n                if (\n                  toolCall.function?.name != null &&\n                  toolCall.function?.arguments != null &&\n                  isParsableJson(toolCall.function.arguments)\n                ) {\n                  controller.enqueue({\n                    type: 'tool-call',\n                    toolCallType: 'function',\n                    toolCallId: toolCall.id ?? generateId(),\n                    toolName: toolCall.function.name,\n                    args: toolCall.function.arguments,\n                  });\n                  toolCall.hasFinished = true;\n                }\n              }\n            }\n          },\n\n          flush(controller) {\n            const metadata = metadataExtractor?.buildMetadata();\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage: {\n                promptTokens: usage.promptTokens ?? NaN,\n                completionTokens: usage.completionTokens ?? NaN,\n              },\n              ...(metadata && { providerMetadata: metadata }),\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body },\n    };\n  }\n}\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst OpenAICompatibleChatResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal('assistant').nullish(),\n        content: z.string().nullish(),\n        reasoning_content: z.string().nullish(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string().nullish(),\n              type: z.literal('function'),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            }),\n          )\n          .nullish(),\n      }),\n      finish_reason: z.string().nullish(),\n    }),\n  ),\n  usage: z\n    .object({\n      prompt_tokens: z.number().nullish(),\n      completion_tokens: z.number().nullish(),\n    })\n    .nullish(),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleChatChunkSchema = <ERROR_SCHEMA extends z.ZodType>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          delta: z\n            .object({\n              role: z.enum(['assistant']).nullish(),\n              content: z.string().nullish(),\n              reasoning_content: z.string().nullish(),\n              tool_calls: z\n                .array(\n                  z.object({\n                    index: z.number(),\n                    id: z.string().nullish(),\n                    type: z.literal('function').optional(),\n                    function: z.object({\n                      name: z.string().nullish(),\n                      arguments: z.string().nullish(),\n                    }),\n                  }),\n                )\n                .nullish(),\n            })\n            .nullish(),\n          finish_reason: z.string().nullish(),\n        }),\n      ),\n      usage: z\n        .object({\n          prompt_tokens: z.number().nullish(),\n          completion_tokens: z.number().nullish(),\n        })\n        .nullish(),\n    }),\n    errorSchema,\n  ]);\n","import {\n  LanguageModelV1Prompt,\n  LanguageModelV1ProviderMetadata,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { convertUint8ArrayToBase64 } from '@ai-sdk/provider-utils';\nimport { OpenAICompatibleChatPrompt } from './openai-compatible-api-types';\n\nfunction getOpenAIMetadata(message: {\n  providerMetadata?: LanguageModelV1ProviderMetadata;\n}) {\n  return message?.providerMetadata?.openaiCompatible ?? {};\n}\n\nexport function convertToOpenAICompatibleChatMessages(\n  prompt: LanguageModelV1Prompt,\n): OpenAICompatibleChatPrompt {\n  const messages: OpenAICompatibleChatPrompt = [];\n  for (const { role, content, ...message } of prompt) {\n    const metadata = getOpenAIMetadata({ ...message });\n    switch (role) {\n      case 'system': {\n        messages.push({ role: 'system', content, ...metadata });\n        break;\n      }\n\n      case 'user': {\n        if (content.length === 1 && content[0].type === 'text') {\n          messages.push({\n            role: 'user',\n            content: content[0].text,\n            ...getOpenAIMetadata(content[0]),\n          });\n          break;\n        }\n\n        messages.push({\n          role: 'user',\n          content: content.map(part => {\n            const partMetadata = getOpenAIMetadata(part);\n            switch (part.type) {\n              case 'text': {\n                return { type: 'text', text: part.text, ...partMetadata };\n              }\n              case 'image': {\n                return {\n                  type: 'image_url',\n                  image_url: {\n                    url:\n                      part.image instanceof URL\n                        ? part.image.toString()\n                        : `data:${\n                            part.mimeType ?? 'image/jpeg'\n                          };base64,${convertUint8ArrayToBase64(part.image)}`,\n                  },\n                  ...partMetadata,\n                };\n              }\n              case 'file': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'File content parts in user messages',\n                });\n              }\n            }\n          }),\n          ...metadata,\n        });\n\n        break;\n      }\n\n      case 'assistant': {\n        let text = '';\n        const toolCalls: Array<{\n          id: string;\n          type: 'function';\n          function: { name: string; arguments: string };\n        }> = [];\n\n        for (const part of content) {\n          const partMetadata = getOpenAIMetadata(part);\n          switch (part.type) {\n            case 'text': {\n              text += part.text;\n              break;\n            }\n            case 'tool-call': {\n              toolCalls.push({\n                id: part.toolCallId,\n                type: 'function',\n                function: {\n                  name: part.toolName,\n                  arguments: JSON.stringify(part.args),\n                },\n                ...partMetadata,\n              });\n              break;\n            }\n          }\n        }\n\n        messages.push({\n          role: 'assistant',\n          content: text,\n          tool_calls: toolCalls.length > 0 ? toolCalls : undefined,\n          ...metadata,\n        });\n\n        break;\n      }\n\n      case 'tool': {\n        for (const toolResponse of content) {\n          const toolResponseMetadata = getOpenAIMetadata(toolResponse);\n          messages.push({\n            role: 'tool',\n            tool_call_id: toolResponse.toolCallId,\n            content: JSON.stringify(toolResponse.result),\n            ...toolResponseMetadata,\n          });\n        }\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return messages;\n}\n","export function getResponseMetadata({\n  id,\n  model,\n  created,\n}: {\n  id?: string | undefined | null;\n  created?: number | undefined | null;\n  model?: string | undefined | null;\n}) {\n  return {\n    id: id ?? undefined,\n    modelId: model ?? undefined,\n    timestamp: created != null ? new Date(created * 1000) : undefined,\n  };\n}\n","import { LanguageModelV1FinishReason } from '@ai-sdk/provider';\n\nexport function mapOpenAICompatibleFinishReason(\n  finishReason: string | null | undefined,\n): LanguageModelV1FinishReason {\n  switch (finishReason) {\n    case 'stop':\n      return 'stop';\n    case 'length':\n      return 'length';\n    case 'content_filter':\n      return 'content-filter';\n    case 'function_call':\n    case 'tool_calls':\n      return 'tool-calls';\n    default:\n      return 'unknown';\n  }\n}\n","import { z, ZodSchema } from 'zod';\n\nexport const openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish(),\n  }),\n});\n\nexport type OpenAICompatibleErrorData = z.infer<\n  typeof openaiCompatibleErrorDataSchema\n>;\n\nexport type ProviderErrorStructure<T> = {\n  errorSchema: ZodSchema<T>;\n  errorToMessage: (error: T) => string;\n  isRetryable?: (response: Response, error?: T) => boolean;\n};\n\nexport const defaultOpenAICompatibleErrorStructure: ProviderErrorStructure<OpenAICompatibleErrorData> =\n  {\n    errorSchema: openaiCompatibleErrorDataSchema,\n    errorToMessage: data => data.error.message,\n  };\n","import {\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function prepareTools({\n  mode,\n  structuredOutputs,\n}: {\n  mode: Parameters<LanguageModelV1['doGenerate']>[0]['mode'] & {\n    type: 'regular';\n  };\n  structuredOutputs: boolean;\n}): {\n  tools:\n    | undefined\n    | Array<{\n        type: 'function';\n        function: {\n          name: string;\n          description: string | undefined;\n          parameters: unknown;\n        };\n      }>;\n  tool_choice:\n    | { type: 'function'; function: { name: string } }\n    | 'auto'\n    | 'none'\n    | 'required'\n    | undefined;\n  toolWarnings: LanguageModelV1CallWarning[];\n} {\n  // when the tools array is empty, change it to undefined to prevent errors:\n  const tools = mode.tools?.length ? mode.tools : undefined;\n  const toolWarnings: LanguageModelV1CallWarning[] = [];\n\n  if (tools == null) {\n    return { tools: undefined, tool_choice: undefined, toolWarnings };\n  }\n\n  const toolChoice = mode.toolChoice;\n\n  const openaiCompatTools: Array<{\n    type: 'function';\n    function: {\n      name: string;\n      description: string | undefined;\n      parameters: unknown;\n    };\n  }> = [];\n\n  for (const tool of tools) {\n    if (tool.type === 'provider-defined') {\n      toolWarnings.push({ type: 'unsupported-tool', tool });\n    } else {\n      openaiCompatTools.push({\n        type: 'function',\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters,\n        },\n      });\n    }\n  }\n\n  if (toolChoice == null) {\n    return { tools: openaiCompatTools, tool_choice: undefined, toolWarnings };\n  }\n\n  const type = toolChoice.type;\n\n  switch (type) {\n    case 'auto':\n    case 'none':\n    case 'required':\n      return { tools: openaiCompatTools, tool_choice: type, toolWarnings };\n    case 'tool':\n      return {\n        tools: openaiCompatTools,\n        tool_choice: {\n          type: 'function',\n          function: {\n            name: toolChoice.toolName,\n          },\n        },\n        toolWarnings,\n      };\n    default: {\n      const _exhaustiveCheck: never = type;\n      throw new UnsupportedFunctionalityError({\n        functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`,\n      });\n    }\n  }\n}\n","import {\n  APICallError,\n  LanguageModelV1,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1StreamPart,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createEventSourceResponseHandler,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  ParseResult,\n  postJsonToApi,\n  ResponseHandler,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { convertToOpenAICompatibleCompletionPrompt } from './convert-to-openai-compatible-completion-prompt';\nimport { getResponseMetadata } from './get-response-metadata';\nimport { mapOpenAICompatibleFinishReason } from './map-openai-compatible-finish-reason';\nimport {\n  OpenAICompatibleCompletionModelId,\n  OpenAICompatibleCompletionSettings,\n} from './openai-compatible-completion-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleCompletionConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n};\n\nexport class OpenAICompatibleCompletionLanguageModel\n  implements LanguageModelV1\n{\n  readonly specificationVersion = 'v1';\n  readonly defaultObjectGenerationMode = undefined;\n\n  readonly modelId: OpenAICompatibleCompletionModelId;\n  readonly settings: OpenAICompatibleCompletionSettings;\n\n  private readonly config: OpenAICompatibleCompletionConfig;\n  private readonly failedResponseHandler: ResponseHandler<APICallError>;\n  private readonly chunkSchema; // type inferred via constructor\n\n  constructor(\n    modelId: OpenAICompatibleCompletionModelId,\n    settings: OpenAICompatibleCompletionSettings,\n    config: OpenAICompatibleCompletionConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n\n    // initialize error handling:\n    const errorStructure =\n      config.errorStructure ?? defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleCompletionChunkSchema(\n      errorStructure.errorSchema,\n    );\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  private get providerOptionsName(): string {\n    return this.config.provider.split('.')[0].trim();\n  }\n\n  private getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n    providerMetadata,\n  }: Parameters<LanguageModelV1['doGenerate']>[0]) {\n    const type = mode.type;\n\n    const warnings: LanguageModelV1CallWarning[] = [];\n\n    if (topK != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'topK',\n      });\n    }\n\n    if (responseFormat != null && responseFormat.type !== 'text') {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'responseFormat',\n        details: 'JSON response format is not supported.',\n      });\n    }\n\n    const { prompt: completionPrompt, stopSequences } =\n      convertToOpenAICompatibleCompletionPrompt({ prompt, inputFormat });\n\n    const stop = [...(stopSequences ?? []), ...(userStopSequences ?? [])];\n\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      ...providerMetadata?.[this.providerOptionsName],\n\n      // prompt:\n      prompt: completionPrompt,\n\n      // stop sequences:\n      stop: stop.length > 0 ? stop : undefined,\n    };\n\n    switch (type) {\n      case 'regular': {\n        if (mode.tools?.length) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'tools',\n          });\n        }\n\n        if (mode.toolChoice) {\n          throw new UnsupportedFunctionalityError({\n            functionality: 'toolChoice',\n          });\n        }\n\n        return { args: baseArgs, warnings };\n      }\n\n      case 'object-json': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-json mode',\n        });\n      }\n\n      case 'object-tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'object-tool mode',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = type;\n        throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  async doGenerate(\n    options: Parameters<LanguageModelV1['doGenerate']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse,\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleCompletionResponseSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n    const choice = response.choices[0];\n\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: response.usage?.prompt_tokens ?? NaN,\n        completionTokens: response.usage?.completion_tokens ?? NaN,\n      },\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders, body: rawResponse },\n      response: getResponseMetadata(response),\n      warnings,\n      request: { body: JSON.stringify(args) },\n    };\n  }\n\n  async doStream(\n    options: Parameters<LanguageModelV1['doStream']>[0],\n  ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n    const { args, warnings } = this.getArgs(options);\n\n    const body = {\n      ...args,\n      stream: true,\n    };\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/completions',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(\n        this.chunkSchema,\n      ),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    const { prompt: rawPrompt, ...rawSettings } = args;\n\n    let finishReason: LanguageModelV1FinishReason = 'unknown';\n    let usage: { promptTokens: number; completionTokens: number } = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN,\n    };\n    let isFirstChunk = true;\n\n    return {\n      stream: response.pipeThrough(\n        new TransformStream<\n          ParseResult<z.infer<typeof this.chunkSchema>>,\n          LanguageModelV1StreamPart\n        >({\n          transform(chunk, controller) {\n            // handle failed chunk parsing / validation:\n            if (!chunk.success) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: chunk.error });\n              return;\n            }\n\n            const value = chunk.value;\n\n            // handle error chunks:\n            if ('error' in value) {\n              finishReason = 'error';\n              controller.enqueue({ type: 'error', error: value.error });\n              return;\n            }\n\n            if (isFirstChunk) {\n              isFirstChunk = false;\n\n              controller.enqueue({\n                type: 'response-metadata',\n                ...getResponseMetadata(value),\n              });\n            }\n\n            if (value.usage != null) {\n              usage = {\n                promptTokens: value.usage.prompt_tokens,\n                completionTokens: value.usage.completion_tokens,\n              };\n            }\n\n            const choice = value.choices[0];\n\n            if (choice?.finish_reason != null) {\n              finishReason = mapOpenAICompatibleFinishReason(\n                choice.finish_reason,\n              );\n            }\n\n            if (choice?.text != null) {\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: choice.text,\n              });\n            }\n          },\n\n          flush(controller) {\n            controller.enqueue({\n              type: 'finish',\n              finishReason,\n              usage,\n            });\n          },\n        }),\n      ),\n      rawCall: { rawPrompt, rawSettings },\n      rawResponse: { headers: responseHeaders },\n      warnings,\n      request: { body: JSON.stringify(body) },\n    };\n  }\n}\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleCompletionResponseSchema = z.object({\n  id: z.string().nullish(),\n  created: z.number().nullish(),\n  model: z.string().nullish(),\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z.string(),\n    }),\n  ),\n  usage: z\n    .object({\n      prompt_tokens: z.number(),\n      completion_tokens: z.number(),\n    })\n    .nullish(),\n});\n\n// limited version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst createOpenAICompatibleCompletionChunkSchema = <\n  ERROR_SCHEMA extends z.ZodType,\n>(\n  errorSchema: ERROR_SCHEMA,\n) =>\n  z.union([\n    z.object({\n      id: z.string().nullish(),\n      created: z.number().nullish(),\n      model: z.string().nullish(),\n      choices: z.array(\n        z.object({\n          text: z.string(),\n          finish_reason: z.string().nullish(),\n          index: z.number(),\n        }),\n      ),\n      usage: z\n        .object({\n          prompt_tokens: z.number(),\n          completion_tokens: z.number(),\n        })\n        .nullish(),\n    }),\n    errorSchema,\n  ]);\n","import {\n  InvalidPromptError,\n  LanguageModelV1Prompt,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport function convertToOpenAICompatibleCompletionPrompt({\n  prompt,\n  inputFormat,\n  user = 'user',\n  assistant = 'assistant',\n}: {\n  prompt: LanguageModelV1Prompt;\n  inputFormat: 'prompt' | 'messages';\n  user?: string;\n  assistant?: string;\n}): {\n  prompt: string;\n  stopSequences?: string[];\n} {\n  // When the user supplied a prompt input, we don't transform it:\n  if (\n    inputFormat === 'prompt' &&\n    prompt.length === 1 &&\n    prompt[0].role === 'user' &&\n    prompt[0].content.length === 1 &&\n    prompt[0].content[0].type === 'text'\n  ) {\n    return { prompt: prompt[0].content[0].text };\n  }\n\n  // otherwise transform to a chat message format:\n  let text = '';\n\n  // if first message is a system message, add it to the text:\n  if (prompt[0].role === 'system') {\n    text += `${prompt[0].content}\\n\\n`;\n    prompt = prompt.slice(1);\n  }\n\n  for (const { role, content } of prompt) {\n    switch (role) {\n      case 'system': {\n        throw new InvalidPromptError({\n          message: 'Unexpected system message in prompt: ${content}',\n          prompt,\n        });\n      }\n\n      case 'user': {\n        const userMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'image': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'images',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${user}:\\n${userMessage}\\n\\n`;\n        break;\n      }\n\n      case 'assistant': {\n        const assistantMessage = content\n          .map(part => {\n            switch (part.type) {\n              case 'text': {\n                return part.text;\n              }\n              case 'tool-call': {\n                throw new UnsupportedFunctionalityError({\n                  functionality: 'tool-call messages',\n                });\n              }\n            }\n          })\n          .join('');\n\n        text += `${assistant}:\\n${assistantMessage}\\n\\n`;\n        break;\n      }\n\n      case 'tool': {\n        throw new UnsupportedFunctionalityError({\n          functionality: 'tool messages',\n        });\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  // Assistant message prefix:\n  text += `${assistant}:\\n`;\n\n  return {\n    prompt: text,\n    stopSequences: [`\\n${user}:`],\n  };\n}\n","import {\n  EmbeddingModelV1,\n  TooManyEmbeddingValuesForCallError,\n} from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport {\n  OpenAICompatibleEmbeddingModelId,\n  OpenAICompatibleEmbeddingSettings,\n} from './openai-compatible-embedding-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\ntype OpenAICompatibleEmbeddingConfig = {\n  /**\nOverride the maximum number of embeddings per call.\n   */\n  maxEmbeddingsPerCall?: number;\n\n  /**\nOverride the parallelism of embedding calls.\n  */\n  supportsParallelCalls?: boolean;\n\n  provider: string;\n  url: (options: { modelId: string; path: string }) => string;\n  headers: () => Record<string, string | undefined>;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n};\n\nexport class OpenAICompatibleEmbeddingModel\n  implements EmbeddingModelV1<string>\n{\n  readonly specificationVersion = 'v1';\n  readonly modelId: OpenAICompatibleEmbeddingModelId;\n\n  private readonly config: OpenAICompatibleEmbeddingConfig;\n  private readonly settings: OpenAICompatibleEmbeddingSettings;\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  get maxEmbeddingsPerCall(): number {\n    return this.config.maxEmbeddingsPerCall ?? 2048;\n  }\n\n  get supportsParallelCalls(): boolean {\n    return this.config.supportsParallelCalls ?? true;\n  }\n\n  constructor(\n    modelId: OpenAICompatibleEmbeddingModelId,\n    settings: OpenAICompatibleEmbeddingSettings,\n    config: OpenAICompatibleEmbeddingConfig,\n  ) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n\n  async doEmbed({\n    values,\n    headers,\n    abortSignal,\n  }: Parameters<EmbeddingModelV1<string>['doEmbed']>[0]): Promise<\n    Awaited<ReturnType<EmbeddingModelV1<string>['doEmbed']>>\n  > {\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values,\n      });\n    }\n\n    const { responseHeaders, value: response } = await postJsonToApi({\n      url: this.config.url({\n        path: '/embeddings',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: 'float',\n        dimensions: this.settings.dimensions,\n        user: this.settings.user,\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiTextEmbeddingResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage\n        ? { tokens: response.usage.prompt_tokens }\n        : undefined,\n      rawResponse: { headers: responseHeaders },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiTextEmbeddingResponseSchema = z.object({\n  data: z.array(z.object({ embedding: z.array(z.number()) })),\n  usage: z.object({ prompt_tokens: z.number() }).nullish(),\n});\n","import { ImageModelV1, ImageModelV1CallWarning } from '@ai-sdk/provider';\nimport {\n  combineHeaders,\n  createJsonErrorResponseHandler,\n  createJsonResponseHandler,\n  FetchFunction,\n  postJsonToApi,\n} from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { OpenAICompatibleImageModelId } from './openai-compatible-image-settings';\nimport { OpenAICompatibleImageSettings } from './openai-compatible-image-settings';\nimport {\n  defaultOpenAICompatibleErrorStructure,\n  ProviderErrorStructure,\n} from './openai-compatible-error';\n\nexport type OpenAICompatibleImageModelConfig = {\n  provider: string;\n  headers: () => Record<string, string | undefined>;\n  url: (options: { modelId: string; path: string }) => string;\n  fetch?: FetchFunction;\n  errorStructure?: ProviderErrorStructure<any>;\n  _internal?: {\n    currentDate?: () => Date;\n  };\n};\n\nexport class OpenAICompatibleImageModel implements ImageModelV1 {\n  readonly specificationVersion = 'v1';\n\n  get maxImagesPerCall(): number {\n    return this.settings.maxImagesPerCall ?? 10;\n  }\n\n  get provider(): string {\n    return this.config.provider;\n  }\n\n  constructor(\n    readonly modelId: OpenAICompatibleImageModelId,\n    private readonly settings: OpenAICompatibleImageSettings,\n    private readonly config: OpenAICompatibleImageModelConfig,\n  ) {}\n\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal,\n  }: Parameters<ImageModelV1['doGenerate']>[0]): Promise<\n    Awaited<ReturnType<ImageModelV1['doGenerate']>>\n  > {\n    const warnings: Array<ImageModelV1CallWarning> = [];\n\n    if (aspectRatio != null) {\n      warnings.push({\n        type: 'unsupported-setting',\n        setting: 'aspectRatio',\n        details:\n          'This model does not support aspect ratio. Use `size` instead.',\n      });\n    }\n\n    if (seed != null) {\n      warnings.push({ type: 'unsupported-setting', setting: 'seed' });\n    }\n\n    const currentDate = this.config._internal?.currentDate?.() ?? new Date();\n    const { value: response, responseHeaders } = await postJsonToApi({\n      url: this.config.url({\n        path: '/images/generations',\n        modelId: this.modelId,\n      }),\n      headers: combineHeaders(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...(providerOptions.openai ?? {}),\n        response_format: 'b64_json',\n        ...(this.settings.user ? { user: this.settings.user } : {}),\n      },\n      failedResponseHandler: createJsonErrorResponseHandler(\n        this.config.errorStructure ?? defaultOpenAICompatibleErrorStructure,\n      ),\n      successfulResponseHandler: createJsonResponseHandler(\n        openaiCompatibleImageResponseSchema,\n      ),\n      abortSignal,\n      fetch: this.config.fetch,\n    });\n\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders,\n      },\n    };\n  }\n}\n\n// minimal version of the schema, focussed on what is needed for the implementation\n// this approach limits breakages when the API changes and increases efficiency\nconst openaiCompatibleImageResponseSchema = z.object({\n  data: z.array(z.object({ b64_json: z.string() })),\n});\n","import {\n  EmbeddingModelV1,\n  ImageModelV1,\n  LanguageModelV1,\n  ProviderV1,\n} from '@ai-sdk/provider';\nimport { FetchFunction, withoutTrailingSlash } from '@ai-sdk/provider-utils';\nimport { OpenAICompatibleChatLanguageModel } from './openai-compatible-chat-language-model';\nimport { OpenAICompatibleChatSettings } from './openai-compatible-chat-settings';\nimport { OpenAICompatibleCompletionLanguageModel } from './openai-compatible-completion-language-model';\nimport { OpenAICompatibleCompletionSettings } from './openai-compatible-completion-settings';\nimport { OpenAICompatibleEmbeddingModel } from './openai-compatible-embedding-model';\nimport { OpenAICompatibleEmbeddingSettings } from './openai-compatible-embedding-settings';\nimport { OpenAICompatibleImageSettings } from './openai-compatible-image-settings';\nimport { OpenAICompatibleImageModel } from './openai-compatible-image-model';\n\nexport interface OpenAICompatibleProvider<\n  CHAT_MODEL_IDS extends string = string,\n  COMPLETION_MODEL_IDS extends string = string,\n  EMBEDDING_MODEL_IDS extends string = string,\n  IMAGE_MODEL_IDS extends string = string,\n> extends Omit<ProviderV1, 'imageModel'> {\n  (\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n  ): LanguageModelV1;\n\n  languageModel(\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n  ): LanguageModelV1;\n\n  chatModel(\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n  ): LanguageModelV1;\n\n  completionModel(\n    modelId: COMPLETION_MODEL_IDS,\n    settings?: OpenAICompatibleCompletionSettings,\n  ): LanguageModelV1;\n\n  textEmbeddingModel(\n    modelId: EMBEDDING_MODEL_IDS,\n    settings?: OpenAICompatibleEmbeddingSettings,\n  ): EmbeddingModelV1<string>;\n\n  imageModel(\n    modelId: IMAGE_MODEL_IDS,\n    settings?: OpenAICompatibleImageSettings,\n  ): ImageModelV1;\n}\n\nexport interface OpenAICompatibleProviderSettings {\n  /**\nBase URL for the API calls.\n   */\n  baseURL: string;\n\n  /**\nProvider name.\n   */\n  name: string;\n\n  /**\nAPI key for authenticating requests. If specified, adds an `Authorization`\nheader to request headers with the value `Bearer <apiKey>`. This will be added\nbefore any headers potentially specified in the `headers` option.\n   */\n  apiKey?: string;\n\n  /**\nOptional custom headers to include in requests. These will be added to request headers\nafter any headers potentially added by use of the `apiKey` option.\n   */\n  headers?: Record<string, string>;\n\n  /**\nOptional custom url query parameters to include in request urls.\n   */\n  queryParams?: Record<string, string>;\n\n  /**\nCustom fetch implementation. You can use it as a middleware to intercept requests,\nor to provide a custom fetch implementation for e.g. testing.\n   */\n  fetch?: FetchFunction;\n}\n\n/**\nCreate an OpenAICompatible provider instance.\n */\nexport function createOpenAICompatible<\n  CHAT_MODEL_IDS extends string,\n  COMPLETION_MODEL_IDS extends string,\n  EMBEDDING_MODEL_IDS extends string,\n  IMAGE_MODEL_IDS extends string,\n>(\n  options: OpenAICompatibleProviderSettings,\n): OpenAICompatibleProvider<\n  CHAT_MODEL_IDS,\n  COMPLETION_MODEL_IDS,\n  EMBEDDING_MODEL_IDS,\n  IMAGE_MODEL_IDS\n> {\n  const baseURL = withoutTrailingSlash(options.baseURL);\n  const providerName = options.name;\n\n  interface CommonModelConfig {\n    provider: string;\n    url: ({ path }: { path: string }) => string;\n    headers: () => Record<string, string>;\n    fetch?: FetchFunction;\n  }\n\n  const getHeaders = () => ({\n    ...(options.apiKey && { Authorization: `Bearer ${options.apiKey}` }),\n    ...options.headers,\n  });\n\n  const getCommonModelConfig = (modelType: string): CommonModelConfig => ({\n    provider: `${providerName}.${modelType}`,\n    url: ({ path }) => {\n      const url = new URL(`${baseURL}${path}`);\n      if (options.queryParams) {\n        url.search = new URLSearchParams(options.queryParams).toString();\n      }\n      return url.toString();\n    },\n    headers: getHeaders,\n    fetch: options.fetch,\n  });\n\n  const createLanguageModel = (\n    modelId: CHAT_MODEL_IDS,\n    settings: OpenAICompatibleChatSettings = {},\n  ) => createChatModel(modelId, settings);\n\n  const createChatModel = (\n    modelId: CHAT_MODEL_IDS,\n    settings: OpenAICompatibleChatSettings = {},\n  ) =>\n    new OpenAICompatibleChatLanguageModel(modelId, settings, {\n      ...getCommonModelConfig('chat'),\n      defaultObjectGenerationMode: 'tool',\n    });\n\n  const createCompletionModel = (\n    modelId: COMPLETION_MODEL_IDS,\n    settings: OpenAICompatibleCompletionSettings = {},\n  ) =>\n    new OpenAICompatibleCompletionLanguageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('completion'),\n    );\n\n  const createEmbeddingModel = (\n    modelId: EMBEDDING_MODEL_IDS,\n    settings: OpenAICompatibleEmbeddingSettings = {},\n  ) =>\n    new OpenAICompatibleEmbeddingModel(\n      modelId,\n      settings,\n      getCommonModelConfig('embedding'),\n    );\n\n  const createImageModel = (\n    modelId: IMAGE_MODEL_IDS,\n    settings: OpenAICompatibleImageSettings = {},\n  ) =>\n    new OpenAICompatibleImageModel(\n      modelId,\n      settings,\n      getCommonModelConfig('image'),\n    );\n\n  const provider = (\n    modelId: CHAT_MODEL_IDS,\n    settings?: OpenAICompatibleChatSettings,\n  ) => createLanguageModel(modelId, settings);\n\n  provider.languageModel = createLanguageModel;\n  provider.chatModel = createChatModel;\n  provider.completionModel = createCompletionModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.imageModel = createImageModel;\n\n  return provider as OpenAICompatibleProvider<\n    CHAT_MODEL_IDS,\n    COMPLETION_MODEL_IDS,\n    EMBEDDING_MODEL_IDS,\n    IMAGE_MODEL_IDS\n  >;\n}\n"],"mappings":";AAAA,SAEEA,wBAAA,QAMK;AACP,SACEC,cAAA,EACAC,gCAAA,EACAC,8BAAA,EACAC,yBAAA,EAEAC,UAAA,EACAC,cAAA,EAEAC,aAAA,QAEK;AACP,SAASC,CAAA,IAAAC,EAAA,QAAS;;;ACrBlB,SAGEC,6BAAA,QACK;AACP,SAASC,yBAAA,QAAiC;AAG1C,SAASC,kBAAkBC,OAAA,EAExB;EAVH,IAAAC,EAAA,EAAAC,EAAA;EAWE,QAAOA,EAAA,IAAAD,EAAA,GAAAD,OAAA,oBAAAA,OAAA,CAASG,gBAAA,KAAT,gBAAAF,EAAA,CAA2BG,gBAAA,KAA3B,OAAAF,EAAA,GAA+C,CAAC;AACzD;AAEO,SAASG,sCACdC,MAAA,EAC4B;EAC5B,MAAMC,QAAA,GAAuC,EAAC;EAC9C,WAAW;IAAEC,IAAA;IAAMC,OAAA;IAAS,GAAGT;EAAQ,KAAKM,MAAA,EAAQ;IAClD,MAAMI,QAAA,GAAWX,iBAAA,CAAkB;MAAE,GAAGC;IAAQ,CAAC;IACjD,QAAQQ,IAAA;MACN,KAAK;QAAU;UACbD,QAAA,CAASI,IAAA,CAAK;YAAEH,IAAA,EAAM;YAAUC,OAAA;YAAS,GAAGC;UAAS,CAAC;UACtD;QACF;MAEA,KAAK;QAAQ;UACX,IAAID,OAAA,CAAQG,MAAA,KAAW,KAAKH,OAAA,CAAQ,CAAC,EAAEI,IAAA,KAAS,QAAQ;YACtDN,QAAA,CAASI,IAAA,CAAK;cACZH,IAAA,EAAM;cACNC,OAAA,EAASA,OAAA,CAAQ,CAAC,EAAEK,IAAA;cACpB,GAAGf,iBAAA,CAAkBU,OAAA,CAAQ,CAAC,CAAC;YACjC,CAAC;YACD;UACF;UAEAF,QAAA,CAASI,IAAA,CAAK;YACZH,IAAA,EAAM;YACNC,OAAA,EAASA,OAAA,CAAQM,GAAA,CAAIC,IAAA,IAAQ;cAtCvC,IAAAf,EAAA;cAuCY,MAAMgB,YAAA,GAAelB,iBAAA,CAAkBiB,IAAI;cAC3C,QAAQA,IAAA,CAAKH,IAAA;gBACX,KAAK;kBAAQ;oBACX,OAAO;sBAAEA,IAAA,EAAM;sBAAQC,IAAA,EAAME,IAAA,CAAKF,IAAA;sBAAM,GAAGG;oBAAa;kBAC1D;gBACA,KAAK;kBAAS;oBACZ,OAAO;sBACLJ,IAAA,EAAM;sBACNK,SAAA,EAAW;wBACTC,GAAA,EACEH,IAAA,CAAKI,KAAA,YAAiBC,GAAA,GAClBL,IAAA,CAAKI,KAAA,CAAME,QAAA,CAAS,IACpB,SACErB,EAAA,GAAAe,IAAA,CAAKO,QAAA,KAAL,OAAAtB,EAAA,GAAiB,YACnB,WAAWH,yBAAA,CAA0BkB,IAAA,CAAKI,KAAK,CAAC;sBACxD;sBACA,GAAGH;oBACL;kBACF;gBACA,KAAK;kBAAQ;oBACX,MAAM,IAAIpB,6BAAA,CAA8B;sBACtC2B,aAAA,EAAe;oBACjB,CAAC;kBACH;cACF;YACF,CAAC;YACD,GAAGd;UACL,CAAC;UAED;QACF;MAEA,KAAK;QAAa;UAChB,IAAII,IAAA,GAAO;UACX,MAAMW,SAAA,GAID,EAAC;UAEN,WAAWT,IAAA,IAAQP,OAAA,EAAS;YAC1B,MAAMQ,YAAA,GAAelB,iBAAA,CAAkBiB,IAAI;YAC3C,QAAQA,IAAA,CAAKH,IAAA;cACX,KAAK;gBAAQ;kBACXC,IAAA,IAAQE,IAAA,CAAKF,IAAA;kBACb;gBACF;cACA,KAAK;gBAAa;kBAChBW,SAAA,CAAUd,IAAA,CAAK;oBACbe,EAAA,EAAIV,IAAA,CAAKW,UAAA;oBACTd,IAAA,EAAM;oBACNe,QAAA,EAAU;sBACRC,IAAA,EAAMb,IAAA,CAAKc,QAAA;sBACXC,SAAA,EAAWC,IAAA,CAAKC,SAAA,CAAUjB,IAAA,CAAKkB,IAAI;oBACrC;oBACA,GAAGjB;kBACL,CAAC;kBACD;gBACF;YACF;UACF;UAEAV,QAAA,CAASI,IAAA,CAAK;YACZH,IAAA,EAAM;YACNC,OAAA,EAASK,IAAA;YACTqB,UAAA,EAAYV,SAAA,CAAUb,MAAA,GAAS,IAAIa,SAAA,GAAY;YAC/C,GAAGf;UACL,CAAC;UAED;QACF;MAEA,KAAK;QAAQ;UACX,WAAW0B,YAAA,IAAgB3B,OAAA,EAAS;YAClC,MAAM4B,oBAAA,GAAuBtC,iBAAA,CAAkBqC,YAAY;YAC3D7B,QAAA,CAASI,IAAA,CAAK;cACZH,IAAA,EAAM;cACN8B,YAAA,EAAcF,YAAA,CAAaT,UAAA;cAC3BlB,OAAA,EAASuB,IAAA,CAAKC,SAAA,CAAUG,YAAA,CAAaG,MAAM;cAC3C,GAAGF;YACL,CAAC;UACH;UACA;QACF;MAEA;QAAS;UACP,MAAMG,gBAAA,GAA0BhC,IAAA;UAChC,MAAM,IAAIiC,KAAA,CAAM,qBAAqBD,gBAAgB,EAAE;QACzD;IACF;EACF;EAEA,OAAOjC,QAAA;AACT;;;ACpIO,SAASmC,oBAAoB;EAClChB,EAAA;EACAiB,KAAA;EACAC;AACF,GAIG;EACD,OAAO;IACLlB,EAAA,EAAIA,EAAA,WAAAA,EAAA,GAAM;IACVmB,OAAA,EAASF,KAAA,WAAAA,KAAA,GAAS;IAClBG,SAAA,EAAWF,OAAA,IAAW,OAAO,IAAIG,IAAA,CAAKH,OAAA,GAAU,GAAI,IAAI;EAC1D;AACF;;;ACZO,SAASI,gCACdC,YAAA,EAC6B;EAC7B,QAAQA,YAAA;IACN,KAAK;MACH,OAAO;IACT,KAAK;MACH,OAAO;IACT,KAAK;MACH,OAAO;IACT,KAAK;IACL,KAAK;MACH,OAAO;IACT;MACE,OAAO;EACX;AACF;;;AClBA,SAAStD,CAAA,QAAoB;AAEtB,IAAMuD,+BAAA,GAAkCvD,CAAA,CAAEwD,MAAA,CAAO;EACtDC,KAAA,EAAOzD,CAAA,CAAEwD,MAAA,CAAO;IACdnD,OAAA,EAASL,CAAA,CAAE0D,MAAA,CAAO;IAAA;IAAA;IAAA;IAKlBxC,IAAA,EAAMlB,CAAA,CAAE0D,MAAA,CAAO,EAAEC,OAAA,CAAQ;IACzBC,KAAA,EAAO5D,CAAA,CAAE6D,GAAA,CAAI,EAAEF,OAAA,CAAQ;IACvBG,IAAA,EAAM9D,CAAA,CAAE+D,KAAA,CAAM,CAAC/D,CAAA,CAAE0D,MAAA,CAAO,GAAG1D,CAAA,CAAEgE,MAAA,CAAO,CAAC,CAAC,EAAEL,OAAA,CAAQ;EAClD,CAAC;AACH,CAAC;AAYM,IAAMM,qCAAA,GACX;EACEC,WAAA,EAAaX,+BAAA;EACbY,cAAA,EAAgBC,IAAA,IAAQA,IAAA,CAAKX,KAAA,CAAMpD;AACrC;;;AC7BF,SAGEH,6BAAA,IAAAmE,8BAAA,QACK;AAEA,SAASC,aAAa;EAC3BC,IAAA;EACAC;AACF,GAuBE;EAhCF,IAAAlE,EAAA;EAkCE,MAAMmE,KAAA,KAAQnE,EAAA,GAAAiE,IAAA,CAAKE,KAAA,KAAL,gBAAAnE,EAAA,CAAYW,MAAA,IAASsD,IAAA,CAAKE,KAAA,GAAQ;EAChD,MAAMC,YAAA,GAA6C,EAAC;EAEpD,IAAID,KAAA,IAAS,MAAM;IACjB,OAAO;MAAEA,KAAA,EAAO;MAAWE,WAAA,EAAa;MAAWD;IAAa;EAClE;EAEA,MAAME,UAAA,GAAaL,IAAA,CAAKK,UAAA;EAExB,MAAMC,iBAAA,GAOD,EAAC;EAEN,WAAWC,IAAA,IAAQL,KAAA,EAAO;IACxB,IAAIK,IAAA,CAAK5D,IAAA,KAAS,oBAAoB;MACpCwD,YAAA,CAAa1D,IAAA,CAAK;QAAEE,IAAA,EAAM;QAAoB4D;MAAK,CAAC;IACtD,OAAO;MACLD,iBAAA,CAAkB7D,IAAA,CAAK;QACrBE,IAAA,EAAM;QACNe,QAAA,EAAU;UACRC,IAAA,EAAM4C,IAAA,CAAK5C,IAAA;UACX6C,WAAA,EAAaD,IAAA,CAAKC,WAAA;UAClBC,UAAA,EAAYF,IAAA,CAAKE;QACnB;MACF,CAAC;IACH;EACF;EAEA,IAAIJ,UAAA,IAAc,MAAM;IACtB,OAAO;MAAEH,KAAA,EAAOI,iBAAA;MAAmBF,WAAA,EAAa;MAAWD;IAAa;EAC1E;EAEA,MAAMxD,IAAA,GAAO0D,UAAA,CAAW1D,IAAA;EAExB,QAAQA,IAAA;IACN,KAAK;IACL,KAAK;IACL,KAAK;MACH,OAAO;QAAEuD,KAAA,EAAOI,iBAAA;QAAmBF,WAAA,EAAazD,IAAA;QAAMwD;MAAa;IACrE,KAAK;MACH,OAAO;QACLD,KAAA,EAAOI,iBAAA;QACPF,WAAA,EAAa;UACXzD,IAAA,EAAM;UACNe,QAAA,EAAU;YACRC,IAAA,EAAM0C,UAAA,CAAWzC;UACnB;QACF;QACAuC;MACF;IACF;MAAS;QACP,MAAM7B,gBAAA,GAA0B3B,IAAA;QAChC,MAAM,IAAImD,8BAAA,CAA8B;UACtCxC,aAAA,EAAe,iCAAiCgB,gBAAgB;QAClE,CAAC;MACH;EACF;AACF;;;ALvCO,IAAMoC,iCAAA,GAAN,MAAmE;EAAA;EAYxEC,YACEhC,OAAA,EACAiC,QAAA,EACAC,MAAA,EACA;IAfF,KAASC,oBAAA,GAAuB;IA1DlC,IAAA/E,EAAA,EAAAC,EAAA;IA0EI,KAAK2C,OAAA,GAAUA,OAAA;IACf,KAAKiC,QAAA,GAAWA,QAAA;IAChB,KAAKC,MAAA,GAASA,MAAA;IAGd,MAAME,cAAA,IACJhF,EAAA,GAAA8E,MAAA,CAAOE,cAAA,KAAP,OAAAhF,EAAA,GAAyB2D,qCAAA;IAC3B,KAAKsB,WAAA,GAAcC,qCAAA,CACjBF,cAAA,CAAepB,WACjB;IACA,KAAKuB,qBAAA,GAAwB9F,8BAAA,CAA+B2F,cAAc;IAE1E,KAAKI,yBAAA,IAA4BnF,EAAA,GAAA6E,MAAA,CAAOM,yBAAA,KAAP,OAAAnF,EAAA,GAAoC;EACvE;EAEA,IAAIoF,4BAAA,EAA2D;IAC7D,OAAO,KAAKP,MAAA,CAAOO,2BAAA;EACrB;EAEA,IAAIC,SAAA,EAAmB;IACrB,OAAO,KAAKR,MAAA,CAAOQ,QAAA;EACrB;EAEA,IAAYC,oBAAA,EAA8B;IACxC,OAAO,KAAKT,MAAA,CAAOQ,QAAA,CAASE,KAAA,CAAM,GAAG,EAAE,CAAC,EAAEC,IAAA,CAAK;EACjD;EAEQC,QAAQ;IACdzB,IAAA;IACA5D,MAAA;IACAsF,SAAA;IACAC,WAAA;IACAC,IAAA;IACAC,IAAA;IACAC,gBAAA;IACAC,eAAA;IACA9F,gBAAA;IACA+F,aAAA;IACAC,cAAA;IACAC;EACF,GAAiD;IAlHnD,IAAAnG,EAAA,EAAAC,EAAA;IAmHI,MAAMW,IAAA,GAAOqD,IAAA,CAAKrD,IAAA;IAElB,MAAMwF,QAAA,GAAyC,EAAC;IAEhD,IAAIN,IAAA,IAAQ,MAAM;MAChBM,QAAA,CAAS1F,IAAA,CAAK;QACZE,IAAA,EAAM;QACNyF,OAAA,EAAS;MACX,CAAC;IACH;IAEA,KACEH,cAAA,oBAAAA,cAAA,CAAgBtF,IAAA,MAAS,UACzBsF,cAAA,CAAeI,MAAA,IAAU,QACzB,CAAC,KAAKlB,yBAAA,EACN;MACAgB,QAAA,CAAS1F,IAAA,CAAK;QACZE,IAAA,EAAM;QACNyF,OAAA,EAAS;QACTE,OAAA,EACE;MACJ,CAAC;IACH;IAEA,MAAMC,QAAA,GAAW;MAAA;MAEf9D,KAAA,EAAO,KAAKE,OAAA;MAAA;MAGZ6D,IAAA,EAAM,KAAK5B,QAAA,CAAS4B,IAAA;MAAA;MAGpBC,UAAA,EAAYf,SAAA;MACZC,WAAA;MACAe,KAAA,EAAOd,IAAA;MACPe,iBAAA,EAAmBb,gBAAA;MACnBc,gBAAA,EAAkBb,eAAA;MAClBc,eAAA,GACEZ,cAAA,oBAAAA,cAAA,CAAgBtF,IAAA,MAAS,SACrB,KAAKwE,yBAAA,KAA8B,QACnCc,cAAA,CAAeI,MAAA,IAAU,OACvB;QACE1F,IAAA,EAAM;QACNmG,WAAA,EAAa;UACXT,MAAA,EAAQJ,cAAA,CAAeI,MAAA;UACvB1E,IAAA,GAAM5B,EAAA,GAAAkG,cAAA,CAAetE,IAAA,KAAf,OAAA5B,EAAA,GAAuB;UAC7ByE,WAAA,EAAayB,cAAA,CAAezB;QAC9B;MACF,IACA;QAAE7D,IAAA,EAAM;MAAc,IACxB;MAENoG,IAAA,EAAMf,aAAA;MACNE,IAAA;MACA,IAAGjG,gBAAA,oBAAAA,gBAAA,CAAmB,KAAKqF,mBAAA;MAAA;MAG3BjF,QAAA,EAAUF,qCAAA,CAAsCC,MAAM;IACxD;IAEA,QAAQO,IAAA;MACN,KAAK;QAAW;UACd,MAAM;YAAEuD,KAAA;YAAOE,WAAA;YAAaD;UAAa,IAAIJ,YAAA,CAAa;YACxDC,IAAA;YACAC,iBAAA,EAAmB,KAAKkB;UAC1B,CAAC;UAED,OAAO;YACLnD,IAAA,EAAM;cAAE,GAAGuE,QAAA;cAAUrC,KAAA;cAAOE;YAAY;YACxC+B,QAAA,EAAU,CAAC,GAAGA,QAAA,EAAU,GAAGhC,YAAY;UACzC;QACF;MAEA,KAAK;QAAe;UAClB,OAAO;YACLnC,IAAA,EAAM;cACJ,GAAGuE,QAAA;cACHM,eAAA,EACE,KAAK1B,yBAAA,KAA8B,QAAQnB,IAAA,CAAKqC,MAAA,IAAU,OACtD;gBACE1F,IAAA,EAAM;gBACNmG,WAAA,EAAa;kBACXT,MAAA,EAAQrC,IAAA,CAAKqC,MAAA;kBACb1E,IAAA,GAAM3B,EAAA,GAAAgE,IAAA,CAAKrC,IAAA,KAAL,OAAA3B,EAAA,GAAa;kBACnBwE,WAAA,EAAaR,IAAA,CAAKQ;gBACpB;cACF,IACA;gBAAE7D,IAAA,EAAM;cAAc;YAC9B;YACAwF;UACF;QACF;MAEA,KAAK;QAAe;UAClB,OAAO;YACLnE,IAAA,EAAM;cACJ,GAAGuE,QAAA;cACHnC,WAAA,EAAa;gBACXzD,IAAA,EAAM;gBACNe,QAAA,EAAU;kBAAEC,IAAA,EAAMqC,IAAA,CAAKO,IAAA,CAAK5C;gBAAK;cACnC;cACAuC,KAAA,EAAO,CACL;gBACEvD,IAAA,EAAM;gBACNe,QAAA,EAAU;kBACRC,IAAA,EAAMqC,IAAA,CAAKO,IAAA,CAAK5C,IAAA;kBAChB6C,WAAA,EAAaR,IAAA,CAAKO,IAAA,CAAKC,WAAA;kBACvBC,UAAA,EAAYT,IAAA,CAAKO,IAAA,CAAKE;gBACxB;cACF;YAEJ;YACA0B;UACF;QACF;MAEA;QAAS;UACP,MAAM7D,gBAAA,GAA0B3B,IAAA;UAChC,MAAM,IAAI4B,KAAA,CAAM,qBAAqBD,gBAAgB,EAAE;QACzD;IACF;EACF;EAEA,MAAM0E,WACJC,OAAA,EAC6D;IAhPjE,IAAAlH,EAAA,EAAAC,EAAA,EAAAkH,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA;IAiPI,MAAM;MAAExF,IAAA;MAAMmE;IAAS,IAAI,KAAKV,OAAA,CAAQ;MAAE,GAAGwB;IAAQ,CAAC;IAEtD,MAAMQ,IAAA,GAAO3F,IAAA,CAAKC,SAAA,CAAUC,IAAI;IAEhC,MAAM;MACJ0F,eAAA;MACAC,KAAA,EAAOC,YAAA;MACPC,QAAA,EAAUC;IACZ,IAAI,MAAMtI,aAAA,CAAc;MACtByB,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAAS9I,cAAA,CAAe,KAAK2F,MAAA,CAAOmD,OAAA,CAAQ,GAAGf,OAAA,CAAQe,OAAO;MAC9DP,IAAA,EAAMzF,IAAA;MACNkD,qBAAA,EAAuB,KAAKA,qBAAA;MAC5B+C,yBAAA,EAA2B5I,yBAAA,CACzB6I,kCACF;MACAC,WAAA,EAAalB,OAAA,CAAQkB,WAAA;MACrBC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,MAAM;MAAE/H,QAAA,EAAUgI,SAAA;MAAW,GAAGC;IAAY,IAAItG,IAAA;IAChD,MAAMuG,MAAA,GAASX,YAAA,CAAaY,OAAA,CAAQ,CAAC;IACrC,MAAMvI,gBAAA,IAAmBD,EAAA,IAAAD,EAAA,QAAK8E,MAAA,CAAO4D,iBAAA,KAAZ,gBAAA1I,EAAA,CAA+B2I,eAAA,KAA/B,gBAAA1I,EAAA,CAAA2I,IAAA,CAAA5I,EAAA,EAAiD;MACxE6I,UAAA,EAAYd;IACd;IAEA,OAAO;MACLlH,IAAA,GAAMsG,EAAA,GAAAqB,MAAA,CAAOzI,OAAA,CAAQS,OAAA,KAAf,OAAA2G,EAAA,GAA0B;MAChC2B,SAAA,GAAW1B,EAAA,GAAAoB,MAAA,CAAOzI,OAAA,CAAQgJ,iBAAA,KAAf,OAAA3B,EAAA,GAAoC;MAC/C5F,SAAA,GAAW6F,EAAA,GAAAmB,MAAA,CAAOzI,OAAA,CAAQmC,UAAA,KAAf,gBAAAmF,EAAA,CAA2BvG,GAAA,CAAIkI,QAAA,IAAS;QAjRzD,IAAAC,GAAA;QAiR6D;UACrDC,YAAA,EAAc;UACdxH,UAAA,GAAYuH,GAAA,GAAAD,QAAA,CAASvH,EAAA,KAAT,OAAAwH,GAAA,GAAe1J,UAAA,CAAW;UACtCsC,QAAA,EAAUmH,QAAA,CAASrH,QAAA,CAASC,IAAA;UAC5BK,IAAA,EAAM+G,QAAA,CAASrH,QAAA,CAASG;QAC1B;MAAA;MACAkB,YAAA,EAAcD,+BAAA,CAAgCyF,MAAA,CAAOW,aAAa;MAClEC,KAAA,EAAO;QACLC,YAAA,GAAc9B,EAAA,IAAAD,EAAA,GAAAO,YAAA,CAAauB,KAAA,KAAb,gBAAA9B,EAAA,CAAoBgC,aAAA,KAApB,OAAA/B,EAAA,GAAqCgC,GAAA;QACnDC,gBAAA,GAAkB/B,EAAA,IAAAD,EAAA,GAAAK,YAAA,CAAauB,KAAA,KAAb,gBAAA5B,EAAA,CAAoBiC,iBAAA,KAApB,OAAAhC,EAAA,GAAyC8B;MAC7D;MACA,IAAIrJ,gBAAA,IAAoB;QAAEA;MAAiB;MAC3CwJ,OAAA,EAAS;QAAEpB,SAAA;QAAWC;MAAY;MAClCR,WAAA,EAAa;QAAEE,OAAA,EAASN,eAAA;QAAiBD,IAAA,EAAMK;MAAY;MAC3D4B,QAAA,EAAUlH,mBAAA,CAAoBoF,YAAY;MAC1CzB,QAAA;MACAwD,OAAA,EAAS;QAAElC;MAAK;IAClB;EACF;EAEA,MAAMmC,SACJ3C,OAAA,EAC2D;IAvS/D,IAAAlH,EAAA;IAwSI,IAAI,KAAK6E,QAAA,CAASiF,iBAAA,EAAmB;MACnC,MAAMxH,MAAA,GAAS,MAAM,KAAK2E,UAAA,CAAWC,OAAO;MAC5C,MAAM6C,eAAA,GAAkB,IAAIC,cAAA,CAA0C;QACpEC,MAAMC,UAAA,EAAY;UAChBA,UAAA,CAAWC,OAAA,CAAQ;YAAEvJ,IAAA,EAAM;YAAqB,GAAG0B,MAAA,CAAOqH;UAAS,CAAC;UACpE,IAAIrH,MAAA,CAAOwG,SAAA,EAAW;YACpB,IAAIsB,KAAA,CAAMC,OAAA,CAAQ/H,MAAA,CAAOwG,SAAS,GAAG;cACnC,WAAW/H,IAAA,IAAQuB,MAAA,CAAOwG,SAAA,EAAW;gBACnC,IAAI/H,IAAA,CAAKH,IAAA,KAAS,QAAQ;kBACxBsJ,UAAA,CAAWC,OAAA,CAAQ;oBACjBvJ,IAAA,EAAM;oBACN0J,SAAA,EAAWvJ,IAAA,CAAKF;kBAClB,CAAC;gBACH;cACF;YACF,OAAO;cACLqJ,UAAA,CAAWC,OAAA,CAAQ;gBACjBvJ,IAAA,EAAM;gBACN0J,SAAA,EAAWhI,MAAA,CAAOwG;cACpB,CAAC;YACH;UACF;UACA,IAAIxG,MAAA,CAAOzB,IAAA,EAAM;YACfqJ,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN0J,SAAA,EAAWhI,MAAA,CAAOzB;YACpB,CAAC;UACH;UACA,IAAIyB,MAAA,CAAOd,SAAA,EAAW;YACpB,WAAWwH,QAAA,IAAY1G,MAAA,CAAOd,SAAA,EAAW;cACvC0I,UAAA,CAAWC,OAAA,CAAQ;gBACjBvJ,IAAA,EAAM;gBACN,GAAGoI;cACL,CAAC;YACH;UACF;UACAkB,UAAA,CAAWC,OAAA,CAAQ;YACjBvJ,IAAA,EAAM;YACNoC,YAAA,EAAcV,MAAA,CAAOU,YAAA;YACrBoG,KAAA,EAAO9G,MAAA,CAAO8G,KAAA;YACdmB,QAAA,EAAUjI,MAAA,CAAOiI,QAAA;YACjBrK,gBAAA,EAAkBoC,MAAA,CAAOpC;UAC3B,CAAC;UACDgK,UAAA,CAAWM,KAAA,CAAM;QACnB;MACF,CAAC;MACD,OAAO;QACLC,MAAA,EAAQV,eAAA;QACRL,OAAA,EAASpH,MAAA,CAAOoH,OAAA;QAChB3B,WAAA,EAAazF,MAAA,CAAOyF,WAAA;QACpB3B,QAAA,EAAU9D,MAAA,CAAO8D;MACnB;IACF;IAEA,MAAM;MAAEnE,IAAA;MAAMmE;IAAS,IAAI,KAAKV,OAAA,CAAQ;MAAE,GAAGwB;IAAQ,CAAC;IAEtD,MAAMQ,IAAA,GAAO3F,IAAA,CAAKC,SAAA,CAAU;MAAE,GAAGC,IAAA;MAAMwI,MAAA,EAAQ;IAAK,CAAC;IACrD,MAAM/B,iBAAA,IACJ1I,EAAA,QAAK8E,MAAA,CAAO4D,iBAAA,KAAZ,gBAAA1I,EAAA,CAA+B0K,qBAAA;IAEjC,MAAM;MAAE/C,eAAA;MAAiBC,KAAA,EAAO+B;IAAS,IAAI,MAAMlK,aAAA,CAAc;MAC/DyB,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAAS9I,cAAA,CAAe,KAAK2F,MAAA,CAAOmD,OAAA,CAAQ,GAAGf,OAAA,CAAQe,OAAO;MAC9DP,IAAA,EAAM;QACJ,GAAGzF,IAAA;QACHwI,MAAA,EAAQ;MACV;MACAtF,qBAAA,EAAuB,KAAKA,qBAAA;MAC5B+C,yBAAA,EAA2B9I,gCAAA,CACzB,KAAK6F,WACP;MACAmD,WAAA,EAAalB,OAAA,CAAQkB,WAAA;MACrBC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,MAAM;MAAE/H,QAAA,EAAUgI,SAAA;MAAW,GAAGC;IAAY,IAAItG,IAAA;IAEhD,MAAMT,SAAA,GAQD,EAAC;IAEN,IAAIwB,YAAA,GAA4C;IAChD,IAAIoG,KAAA,GAGA;MACFC,YAAA,EAAc;MACdG,gBAAA,EAAkB;IACpB;IACA,IAAImB,YAAA,GAAe;IAEnB,OAAO;MACLF,MAAA,EAAQd,QAAA,CAASiB,WAAA,CACf,IAAIC,eAAA,CAGF;QAAA;QAEAC,UAAUC,KAAA,EAAOb,UAAA,EAAY;UAnZvC,IAAAjB,GAAA,EAAAhJ,EAAA,EAAAkH,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAuD,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA,EAAAC,EAAA;UAqZY,IAAI,CAACL,KAAA,CAAMM,OAAA,EAAS;YAClBrI,YAAA,GAAe;YACfkH,UAAA,CAAWC,OAAA,CAAQ;cAAEvJ,IAAA,EAAM;cAASuC,KAAA,EAAO4H,KAAA,CAAM5H;YAAM,CAAC;YACxD;UACF;UACA,MAAMyE,KAAA,GAAQmD,KAAA,CAAMnD,KAAA;UAEpBc,iBAAA,oBAAAA,iBAAA,CAAmB4C,YAAA,CAAaP,KAAA,CAAMjD,QAAA;UAGtC,IAAI,WAAWF,KAAA,EAAO;YACpB5E,YAAA,GAAe;YACfkH,UAAA,CAAWC,OAAA,CAAQ;cAAEvJ,IAAA,EAAM;cAASuC,KAAA,EAAOyE,KAAA,CAAMzE,KAAA,CAAMpD;YAAQ,CAAC;YAChE;UACF;UAEA,IAAI4K,YAAA,EAAc;YAChBA,YAAA,GAAe;YAEfT,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN,GAAG6B,mBAAA,CAAoBmF,KAAK;YAC9B,CAAC;UACH;UAEA,IAAIA,KAAA,CAAMwB,KAAA,IAAS,MAAM;YACvBA,KAAA,GAAQ;cACNC,YAAA,GAAcJ,GAAA,GAAArB,KAAA,CAAMwB,KAAA,CAAME,aAAA,KAAZ,OAAAL,GAAA,GAA6B;cAC3CO,gBAAA,GAAkBvJ,EAAA,GAAA2H,KAAA,CAAMwB,KAAA,CAAMK,iBAAA,KAAZ,OAAAxJ,EAAA,GAAiC;YACrD;UACF;UAEA,MAAMuI,MAAA,GAASZ,KAAA,CAAMa,OAAA,CAAQ,CAAC;UAE9B,KAAID,MAAA,oBAAAA,MAAA,CAAQW,aAAA,KAAiB,MAAM;YACjCnG,YAAA,GAAeD,+BAAA,CACbyF,MAAA,CAAOW,aACT;UACF;UAEA,KAAIX,MAAA,oBAAAA,MAAA,CAAQ+C,KAAA,KAAS,MAAM;YACzB;UACF;UAEA,MAAMA,KAAA,GAAQ/C,MAAA,CAAO+C,KAAA;UAGrB,IAAIA,KAAA,CAAMxC,iBAAA,IAAqB,MAAM;YACnCmB,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN0J,SAAA,EAAWiB,KAAA,CAAMxC;YACnB,CAAC;UACH;UAEA,IAAIwC,KAAA,CAAM/K,OAAA,IAAW,MAAM;YACzB0J,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN0J,SAAA,EAAWiB,KAAA,CAAM/K;YACnB,CAAC;UACH;UAEA,IAAI+K,KAAA,CAAMrJ,UAAA,IAAc,MAAM;YAC5B,WAAWsJ,aAAA,IAAiBD,KAAA,CAAMrJ,UAAA,EAAY;cAC5C,MAAMuJ,KAAA,GAAQD,aAAA,CAAcC,KAAA;cAE5B,IAAIjK,SAAA,CAAUiK,KAAK,KAAK,MAAM;gBAC5B,IAAID,aAAA,CAAc5K,IAAA,KAAS,YAAY;kBACrC,MAAM,IAAI1B,wBAAA,CAAyB;oBACjC4E,IAAA,EAAM0H,aAAA;oBACNzL,OAAA,EAAS;kBACX,CAAC;gBACH;gBAEA,IAAIyL,aAAA,CAAc/J,EAAA,IAAM,MAAM;kBAC5B,MAAM,IAAIvC,wBAAA,CAAyB;oBACjC4E,IAAA,EAAM0H,aAAA;oBACNzL,OAAA,EAAS;kBACX,CAAC;gBACH;gBAEA,MAAIoH,EAAA,GAAAqE,aAAA,CAAc7J,QAAA,KAAd,gBAAAwF,EAAA,CAAwBvF,IAAA,KAAQ,MAAM;kBACxC,MAAM,IAAI1C,wBAAA,CAAyB;oBACjC4E,IAAA,EAAM0H,aAAA;oBACNzL,OAAA,EAAS;kBACX,CAAC;gBACH;gBAEAyB,SAAA,CAAUiK,KAAK,IAAI;kBACjBhK,EAAA,EAAI+J,aAAA,CAAc/J,EAAA;kBAClBb,IAAA,EAAM;kBACNe,QAAA,EAAU;oBACRC,IAAA,EAAM4J,aAAA,CAAc7J,QAAA,CAASC,IAAA;oBAC7BE,SAAA,GAAWsF,EAAA,GAAAoE,aAAA,CAAc7J,QAAA,CAASG,SAAA,KAAvB,OAAAsF,EAAA,GAAoC;kBACjD;kBACAsE,WAAA,EAAa;gBACf;gBAEA,MAAMC,SAAA,GAAWnK,SAAA,CAAUiK,KAAK;gBAEhC,MACEpE,EAAA,GAAAsE,SAAA,CAAShK,QAAA,KAAT,gBAAA0F,EAAA,CAAmBzF,IAAA,KAAQ,UAC3B0F,EAAA,GAAAqE,SAAA,CAAShK,QAAA,KAAT,gBAAA2F,EAAA,CAAmBxF,SAAA,KAAa,MAChC;kBAEA,IAAI6J,SAAA,CAAShK,QAAA,CAASG,SAAA,CAAUnB,MAAA,GAAS,GAAG;oBAC1CuJ,UAAA,CAAWC,OAAA,CAAQ;sBACjBvJ,IAAA,EAAM;sBACNsI,YAAA,EAAc;sBACdxH,UAAA,EAAYiK,SAAA,CAASlK,EAAA;sBACrBI,QAAA,EAAU8J,SAAA,CAAShK,QAAA,CAASC,IAAA;sBAC5BgK,aAAA,EAAeD,SAAA,CAAShK,QAAA,CAASG;oBACnC,CAAC;kBACH;kBAIA,IAAItC,cAAA,CAAemM,SAAA,CAAShK,QAAA,CAASG,SAAS,GAAG;oBAC/CoI,UAAA,CAAWC,OAAA,CAAQ;sBACjBvJ,IAAA,EAAM;sBACNsI,YAAA,EAAc;sBACdxH,UAAA,GAAY6F,EAAA,GAAAoE,SAAA,CAASlK,EAAA,KAAT,OAAA8F,EAAA,GAAehI,UAAA,CAAW;sBACtCsC,QAAA,EAAU8J,SAAA,CAAShK,QAAA,CAASC,IAAA;sBAC5BK,IAAA,EAAM0J,SAAA,CAAShK,QAAA,CAASG;oBAC1B,CAAC;oBACD6J,SAAA,CAASD,WAAA,GAAc;kBACzB;gBACF;gBAEA;cACF;cAGA,MAAM1C,QAAA,GAAWxH,SAAA,CAAUiK,KAAK;cAEhC,IAAIzC,QAAA,CAAS0C,WAAA,EAAa;gBACxB;cACF;cAEA,MAAIlE,EAAA,GAAAgE,aAAA,CAAc7J,QAAA,KAAd,gBAAA6F,EAAA,CAAwB1F,SAAA,KAAa,MAAM;gBAC7CkH,QAAA,CAASrH,QAAA,CAAUG,SAAA,KACjBkJ,EAAA,IAAAvD,EAAA,GAAA+D,aAAA,CAAc7J,QAAA,KAAd,gBAAA8F,EAAA,CAAwB3F,SAAA,KAAxB,OAAAkJ,EAAA,GAAqC;cACzC;cAGAd,UAAA,CAAWC,OAAA,CAAQ;gBACjBvJ,IAAA,EAAM;gBACNsI,YAAA,EAAc;gBACdxH,UAAA,EAAYsH,QAAA,CAASvH,EAAA;gBACrBI,QAAA,EAAUmH,QAAA,CAASrH,QAAA,CAASC,IAAA;gBAC5BgK,aAAA,GAAeX,EAAA,GAAAO,aAAA,CAAc7J,QAAA,CAASG,SAAA,KAAvB,OAAAmJ,EAAA,GAAoC;cACrD,CAAC;cAGD,MACEC,EAAA,GAAAlC,QAAA,CAASrH,QAAA,KAAT,gBAAAuJ,EAAA,CAAmBtJ,IAAA,KAAQ,UAC3BuJ,EAAA,GAAAnC,QAAA,CAASrH,QAAA,KAAT,gBAAAwJ,EAAA,CAAmBrJ,SAAA,KAAa,QAChCtC,cAAA,CAAewJ,QAAA,CAASrH,QAAA,CAASG,SAAS,GAC1C;gBACAoI,UAAA,CAAWC,OAAA,CAAQ;kBACjBvJ,IAAA,EAAM;kBACNsI,YAAA,EAAc;kBACdxH,UAAA,GAAY0J,EAAA,GAAApC,QAAA,CAASvH,EAAA,KAAT,OAAA2J,EAAA,GAAe7L,UAAA,CAAW;kBACtCsC,QAAA,EAAUmH,QAAA,CAASrH,QAAA,CAASC,IAAA;kBAC5BK,IAAA,EAAM+G,QAAA,CAASrH,QAAA,CAASG;gBAC1B,CAAC;gBACDkH,QAAA,CAAS0C,WAAA,GAAc;cACzB;YACF;UACF;QACF;QAEAG,MAAM3B,UAAA,EAAY;UAhkB5B,IAAAjB,GAAA,EAAAhJ,EAAA;UAikBY,MAAMQ,QAAA,GAAWiI,iBAAA,oBAAAA,iBAAA,CAAmBoD,aAAA;UACpC5B,UAAA,CAAWC,OAAA,CAAQ;YACjBvJ,IAAA,EAAM;YACNoC,YAAA;YACAoG,KAAA,EAAO;cACLC,YAAA,GAAcJ,GAAA,GAAAG,KAAA,CAAMC,YAAA,KAAN,OAAAJ,GAAA,GAAsBM,GAAA;cACpCC,gBAAA,GAAkBvJ,EAAA,GAAAmJ,KAAA,CAAMI,gBAAA,KAAN,OAAAvJ,EAAA,GAA0BsJ;YAC9C;YACA,IAAI9I,QAAA,IAAY;cAAEP,gBAAA,EAAkBO;YAAS;UAC/C,CAAC;QACH;MACF,CAAC,CACH;MACAiJ,OAAA,EAAS;QAAEpB,SAAA;QAAWC;MAAY;MAClCR,WAAA,EAAa;QAAEE,OAAA,EAASN;MAAgB;MACxCvB,QAAA;MACAwD,OAAA,EAAS;QAAElC;MAAK;IAClB;EACF;AACF;AAIA,IAAMS,kCAAA,GAAqCxI,EAAA,CAAEuD,MAAA,CAAO;EAClDzB,EAAA,EAAI9B,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACvBV,OAAA,EAAShD,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;EAC5BX,KAAA,EAAO/C,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EAC1BoF,OAAA,EAAS9I,EAAA,CAAEoM,KAAA,CACTpM,EAAA,CAAEuD,MAAA,CAAO;IACPnD,OAAA,EAASJ,EAAA,CAAEuD,MAAA,CAAO;MAChB3C,IAAA,EAAMZ,EAAA,CAAEqM,OAAA,CAAQ,WAAW,EAAE3I,OAAA,CAAQ;MACrC7C,OAAA,EAASb,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;MAC5B0F,iBAAA,EAAmBpJ,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;MACtCnB,UAAA,EAAYvC,EAAA,CACToM,KAAA,CACCpM,EAAA,CAAEuD,MAAA,CAAO;QACPzB,EAAA,EAAI9B,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;QACvBzC,IAAA,EAAMjB,EAAA,CAAEqM,OAAA,CAAQ,UAAU;QAC1BrK,QAAA,EAAUhC,EAAA,CAAEuD,MAAA,CAAO;UACjBtB,IAAA,EAAMjC,EAAA,CAAEyD,MAAA,CAAO;UACftB,SAAA,EAAWnC,EAAA,CAAEyD,MAAA,CAAO;QACtB,CAAC;MACH,CAAC,CACH,EACCC,OAAA,CAAQ;IACb,CAAC;IACD8F,aAAA,EAAexJ,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACpC,CAAC,CACH;EACA+F,KAAA,EAAOzJ,EAAA,CACJuD,MAAA,CAAO;IACNoG,aAAA,EAAe3J,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;IAClCoG,iBAAA,EAAmB9J,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;EACxC,CAAC,EACAA,OAAA,CAAQ;AACb,CAAC;AAID,IAAM6B,qCAAA,GACJtB,WAAA,IAEAjE,EAAA,CAAE8D,KAAA,CAAM,CACN9D,EAAA,CAAEuD,MAAA,CAAO;EACPzB,EAAA,EAAI9B,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACvBV,OAAA,EAAShD,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;EAC5BX,KAAA,EAAO/C,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EAC1BoF,OAAA,EAAS9I,EAAA,CAAEoM,KAAA,CACTpM,EAAA,CAAEuD,MAAA,CAAO;IACPqI,KAAA,EAAO5L,EAAA,CACJuD,MAAA,CAAO;MACN3C,IAAA,EAAMZ,EAAA,CAAEsM,IAAA,CAAK,CAAC,WAAW,CAAC,EAAE5I,OAAA,CAAQ;MACpC7C,OAAA,EAASb,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;MAC5B0F,iBAAA,EAAmBpJ,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;MACtCnB,UAAA,EAAYvC,EAAA,CACToM,KAAA,CACCpM,EAAA,CAAEuD,MAAA,CAAO;QACPuI,KAAA,EAAO9L,EAAA,CAAE+D,MAAA,CAAO;QAChBjC,EAAA,EAAI9B,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;QACvBzC,IAAA,EAAMjB,EAAA,CAAEqM,OAAA,CAAQ,UAAU,EAAEE,QAAA,CAAS;QACrCvK,QAAA,EAAUhC,EAAA,CAAEuD,MAAA,CAAO;UACjBtB,IAAA,EAAMjC,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;UACzBvB,SAAA,EAAWnC,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;QAChC,CAAC;MACH,CAAC,CACH,EACCA,OAAA,CAAQ;IACb,CAAC,EACAA,OAAA,CAAQ;IACX8F,aAAA,EAAexJ,EAAA,CAAEyD,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACpC,CAAC,CACH;EACA+F,KAAA,EAAOzJ,EAAA,CACJuD,MAAA,CAAO;IACNoG,aAAA,EAAe3J,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;IAClCoG,iBAAA,EAAmB9J,EAAA,CAAE+D,MAAA,CAAO,EAAEL,OAAA,CAAQ;EACxC,CAAC,EACAA,OAAA,CAAQ;AACb,CAAC,GACDO,WAAA,CACD;;;AMrqBH,SAMEhE,6BAAA,IAAAuM,8BAAA,QACK;AACP,SACEhN,cAAA,IAAAiN,eAAA,EACAhN,gCAAA,IAAAiN,iCAAA,EACAhN,8BAAA,IAAAiN,+BAAA,EACAhN,yBAAA,IAAAiN,0BAAA,EAGA9M,aAAA,IAAA+M,cAAA,QAEK;AACP,SAAS9M,CAAA,IAAA+M,EAAA,QAAS;;;AClBlB,SACEC,kBAAA,EAEA9M,6BAAA,IAAA+M,8BAAA,QACK;AAEA,SAASC,0CAA0C;EACxDvM,MAAA;EACAwM,WAAA;EACApG,IAAA,GAAO;EACPqG,SAAA,GAAY;AACd,GAQE;EAEA,IACED,WAAA,KAAgB,YAChBxM,MAAA,CAAOM,MAAA,KAAW,KAClBN,MAAA,CAAO,CAAC,EAAEE,IAAA,KAAS,UACnBF,MAAA,CAAO,CAAC,EAAEG,OAAA,CAAQG,MAAA,KAAW,KAC7BN,MAAA,CAAO,CAAC,EAAEG,OAAA,CAAQ,CAAC,EAAEI,IAAA,KAAS,QAC9B;IACA,OAAO;MAAEP,MAAA,EAAQA,MAAA,CAAO,CAAC,EAAEG,OAAA,CAAQ,CAAC,EAAEK;IAAK;EAC7C;EAGA,IAAIA,IAAA,GAAO;EAGX,IAAIR,MAAA,CAAO,CAAC,EAAEE,IAAA,KAAS,UAAU;IAC/BM,IAAA,IAAQ,GAAGR,MAAA,CAAO,CAAC,EAAEG,OAAO;AAAA;AAAA;IAC5BH,MAAA,GAASA,MAAA,CAAO0M,KAAA,CAAM,CAAC;EACzB;EAEA,WAAW;IAAExM,IAAA;IAAMC;EAAQ,KAAKH,MAAA,EAAQ;IACtC,QAAQE,IAAA;MACN,KAAK;QAAU;UACb,MAAM,IAAImM,kBAAA,CAAmB;YAC3B3M,OAAA,EAAS;YACTM;UACF,CAAC;QACH;MAEA,KAAK;QAAQ;UACX,MAAM2M,WAAA,GAAcxM,OAAA,CACjBM,GAAA,CAAIC,IAAA,IAAQ;YACX,QAAQA,IAAA,CAAKH,IAAA;cACX,KAAK;gBAAQ;kBACX,OAAOG,IAAA,CAAKF,IAAA;gBACd;cACA,KAAK;gBAAS;kBACZ,MAAM,IAAI8L,8BAAA,CAA8B;oBACtCpL,aAAA,EAAe;kBACjB,CAAC;gBACH;YACF;UACF,CAAC,EACA0L,IAAA,CAAK,EAAE;UAEVpM,IAAA,IAAQ,GAAG4F,IAAI;AAAA,EAAMuG,WAAW;AAAA;AAAA;UAChC;QACF;MAEA,KAAK;QAAa;UAChB,MAAME,gBAAA,GAAmB1M,OAAA,CACtBM,GAAA,CAAIC,IAAA,IAAQ;YACX,QAAQA,IAAA,CAAKH,IAAA;cACX,KAAK;gBAAQ;kBACX,OAAOG,IAAA,CAAKF,IAAA;gBACd;cACA,KAAK;gBAAa;kBAChB,MAAM,IAAI8L,8BAAA,CAA8B;oBACtCpL,aAAA,EAAe;kBACjB,CAAC;gBACH;YACF;UACF,CAAC,EACA0L,IAAA,CAAK,EAAE;UAEVpM,IAAA,IAAQ,GAAGiM,SAAS;AAAA,EAAMI,gBAAgB;AAAA;AAAA;UAC1C;QACF;MAEA,KAAK;QAAQ;UACX,MAAM,IAAIP,8BAAA,CAA8B;YACtCpL,aAAA,EAAe;UACjB,CAAC;QACH;MAEA;QAAS;UACP,MAAMgB,gBAAA,GAA0BhC,IAAA;UAChC,MAAM,IAAIiC,KAAA,CAAM,qBAAqBD,gBAAgB,EAAE;QACzD;IACF;EACF;EAGA1B,IAAA,IAAQ,GAAGiM,SAAS;AAAA;EAEpB,OAAO;IACLzM,MAAA,EAAQQ,IAAA;IACRoF,aAAA,EAAe,CAAC;AAAA,EAAKQ,IAAI,GAAG;EAC9B;AACF;;;ADtEO,IAAM0G,uCAAA,GAAN,MAEP;EAAA;EAWEvI,YACEhC,OAAA,EACAiC,QAAA,EACAC,MAAA,EACA;IAdF,KAASC,oBAAA,GAAuB;IAChC,KAASM,2BAAA,GAA8B;IA3CzC,IAAArF,EAAA;IAyDI,KAAK4C,OAAA,GAAUA,OAAA;IACf,KAAKiC,QAAA,GAAWA,QAAA;IAChB,KAAKC,MAAA,GAASA,MAAA;IAGd,MAAME,cAAA,IACJhF,EAAA,GAAA8E,MAAA,CAAOE,cAAA,KAAP,OAAAhF,EAAA,GAAyB2D,qCAAA;IAC3B,KAAKsB,WAAA,GAAcmI,2CAAA,CACjBpI,cAAA,CAAepB,WACjB;IACA,KAAKuB,qBAAA,GAAwBmH,+BAAA,CAA+BtH,cAAc;EAC5E;EAEA,IAAIM,SAAA,EAAmB;IACrB,OAAO,KAAKR,MAAA,CAAOQ,QAAA;EACrB;EAEA,IAAYC,oBAAA,EAA8B;IACxC,OAAO,KAAKT,MAAA,CAAOQ,QAAA,CAASE,KAAA,CAAM,GAAG,EAAE,CAAC,EAAEC,IAAA,CAAK;EACjD;EAEQC,QAAQ;IACdzB,IAAA;IACA4I,WAAA;IACAxM,MAAA;IACAsF,SAAA;IACAC,WAAA;IACAC,IAAA;IACAC,IAAA;IACAC,gBAAA;IACAC,eAAA;IACAC,aAAA,EAAeoH,iBAAA;IACfnH,cAAA;IACAC,IAAA;IACAjG;EACF,GAAiD;IA5FnD,IAAAF,EAAA;IA6FI,MAAMY,IAAA,GAAOqD,IAAA,CAAKrD,IAAA;IAElB,MAAMwF,QAAA,GAAyC,EAAC;IAEhD,IAAIN,IAAA,IAAQ,MAAM;MAChBM,QAAA,CAAS1F,IAAA,CAAK;QACZE,IAAA,EAAM;QACNyF,OAAA,EAAS;MACX,CAAC;IACH;IAEA,IAAIH,cAAA,IAAkB,QAAQA,cAAA,CAAetF,IAAA,KAAS,QAAQ;MAC5DwF,QAAA,CAAS1F,IAAA,CAAK;QACZE,IAAA,EAAM;QACNyF,OAAA,EAAS;QACTE,OAAA,EAAS;MACX,CAAC;IACH;IAEA,MAAM;MAAElG,MAAA,EAAQiN,gBAAA;MAAkBrH;IAAc,IAC9C2G,yCAAA,CAA0C;MAAEvM,MAAA;MAAQwM;IAAY,CAAC;IAEnE,MAAM7F,IAAA,GAAO,CAAC,IAAIf,aAAA,WAAAA,aAAA,GAAiB,EAAC,GAAI,IAAIoH,iBAAA,WAAAA,iBAAA,GAAqB,EAAG;IAEpE,MAAM7G,QAAA,GAAW;MAAA;MAEf9D,KAAA,EAAO,KAAKE,OAAA;MAAA;MAGZ2K,IAAA,EAAM,KAAK1I,QAAA,CAAS0I,IAAA;MACpBC,UAAA,EAAY,KAAK3I,QAAA,CAAS4I,SAAA;MAC1BC,MAAA,EAAQ,KAAK7I,QAAA,CAAS6I,MAAA;MACtBjH,IAAA,EAAM,KAAK5B,QAAA,CAAS4B,IAAA;MAAA;MAGpBC,UAAA,EAAYf,SAAA;MACZC,WAAA;MACAe,KAAA,EAAOd,IAAA;MACPe,iBAAA,EAAmBb,gBAAA;MACnBc,gBAAA,EAAkBb,eAAA;MAClBG,IAAA;MACA,IAAGjG,gBAAA,oBAAAA,gBAAA,CAAmB,KAAKqF,mBAAA;MAAA;MAG3BlF,MAAA,EAAQiN,gBAAA;MAAA;MAGRtG,IAAA,EAAMA,IAAA,CAAKrG,MAAA,GAAS,IAAIqG,IAAA,GAAO;IACjC;IAEA,QAAQpG,IAAA;MACN,KAAK;QAAW;UACd,KAAIZ,EAAA,GAAAiE,IAAA,CAAKE,KAAA,KAAL,gBAAAnE,EAAA,CAAYW,MAAA,EAAQ;YACtB,MAAM,IAAIwL,8BAAA,CAA8B;cACtC5K,aAAA,EAAe;YACjB,CAAC;UACH;UAEA,IAAI0C,IAAA,CAAKK,UAAA,EAAY;YACnB,MAAM,IAAI6H,8BAAA,CAA8B;cACtC5K,aAAA,EAAe;YACjB,CAAC;UACH;UAEA,OAAO;YAAEU,IAAA,EAAMuE,QAAA;YAAUJ;UAAS;QACpC;MAEA,KAAK;QAAe;UAClB,MAAM,IAAI+F,8BAAA,CAA8B;YACtC5K,aAAA,EAAe;UACjB,CAAC;QACH;MAEA,KAAK;QAAe;UAClB,MAAM,IAAI4K,8BAAA,CAA8B;YACtC5K,aAAA,EAAe;UACjB,CAAC;QACH;MAEA;QAAS;UACP,MAAMgB,gBAAA,GAA0B3B,IAAA;UAChC,MAAM,IAAI4B,KAAA,CAAM,qBAAqBD,gBAAgB,EAAE;QACzD;IACF;EACF;EAEA,MAAM0E,WACJC,OAAA,EAC6D;IArLjE,IAAAlH,EAAA,EAAAC,EAAA,EAAAkH,EAAA,EAAAC,EAAA;IAsLI,MAAM;MAAEnF,IAAA;MAAMmE;IAAS,IAAI,KAAKV,OAAA,CAAQwB,OAAO;IAE/C,MAAM;MACJS,eAAA;MACAC,KAAA,EAAO+B,QAAA;MACP7B,QAAA,EAAUC;IACZ,IAAI,MAAMyE,cAAA,CAAc;MACtBtL,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAASmE,eAAA,CAAe,KAAKtH,MAAA,CAAOmD,OAAA,CAAQ,GAAGf,OAAA,CAAQe,OAAO;MAC9DP,IAAA,EAAMzF,IAAA;MACNkD,qBAAA,EAAuB,KAAKA,qBAAA;MAC5B+C,yBAAA,EAA2BqE,0BAAA,CACzBoB,wCACF;MACAvF,WAAA,EAAalB,OAAA,CAAQkB,WAAA;MACrBC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,MAAM;MAAEhI,MAAA,EAAQiI,SAAA;MAAW,GAAGC;IAAY,IAAItG,IAAA;IAC9C,MAAMuG,MAAA,GAASmB,QAAA,CAASlB,OAAA,CAAQ,CAAC;IAEjC,OAAO;MACL5H,IAAA,EAAM2H,MAAA,CAAO3H,IAAA;MACbuI,KAAA,EAAO;QACLC,YAAA,GAAcpJ,EAAA,IAAAD,EAAA,GAAA2J,QAAA,CAASP,KAAA,KAAT,gBAAApJ,EAAA,CAAgBsJ,aAAA,KAAhB,OAAArJ,EAAA,GAAiCsJ,GAAA;QAC/CC,gBAAA,GAAkBpC,EAAA,IAAAD,EAAA,GAAAwC,QAAA,CAASP,KAAA,KAAT,gBAAAjC,EAAA,CAAgBsC,iBAAA,KAAhB,OAAArC,EAAA,GAAqCmC;MACzD;MACAvG,YAAA,EAAcD,+BAAA,CAAgCyF,MAAA,CAAOW,aAAa;MAClEO,OAAA,EAAS;QAAEpB,SAAA;QAAWC;MAAY;MAClCR,WAAA,EAAa;QAAEE,OAAA,EAASN,eAAA;QAAiBD,IAAA,EAAMK;MAAY;MAC3D4B,QAAA,EAAUlH,mBAAA,CAAoBkH,QAAQ;MACtCvD,QAAA;MACAwD,OAAA,EAAS;QAAElC,IAAA,EAAM3F,IAAA,CAAKC,SAAA,CAAUC,IAAI;MAAE;IACxC;EACF;EAEA,MAAM4H,SACJ3C,OAAA,EAC2D;IAC3D,MAAM;MAAEjF,IAAA;MAAMmE;IAAS,IAAI,KAAKV,OAAA,CAAQwB,OAAO;IAE/C,MAAMQ,IAAA,GAAO;MACX,GAAGzF,IAAA;MACHwI,MAAA,EAAQ;IACV;IAEA,MAAM;MAAE9C,eAAA;MAAiBC,KAAA,EAAO+B;IAAS,IAAI,MAAM6C,cAAA,CAAc;MAC/DtL,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAASmE,eAAA,CAAe,KAAKtH,MAAA,CAAOmD,OAAA,CAAQ,GAAGf,OAAA,CAAQe,OAAO;MAC9DP,IAAA;MACAvC,qBAAA,EAAuB,KAAKA,qBAAA;MAC5B+C,yBAAA,EAA2BmE,iCAAA,CACzB,KAAKpH,WACP;MACAmD,WAAA,EAAalB,OAAA,CAAQkB,WAAA;MACrBC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,MAAM;MAAEhI,MAAA,EAAQiI,SAAA;MAAW,GAAGC;IAAY,IAAItG,IAAA;IAE9C,IAAIe,YAAA,GAA4C;IAChD,IAAIoG,KAAA,GAA4D;MAC9DC,YAAA,EAAcuE,MAAA,CAAOrE,GAAA;MACrBC,gBAAA,EAAkBoE,MAAA,CAAOrE;IAC3B;IACA,IAAIoB,YAAA,GAAe;IAEnB,OAAO;MACLF,MAAA,EAAQd,QAAA,CAASiB,WAAA,CACf,IAAIC,eAAA,CAGF;QACAC,UAAUC,KAAA,EAAOb,UAAA,EAAY;UAE3B,IAAI,CAACa,KAAA,CAAMM,OAAA,EAAS;YAClBrI,YAAA,GAAe;YACfkH,UAAA,CAAWC,OAAA,CAAQ;cAAEvJ,IAAA,EAAM;cAASuC,KAAA,EAAO4H,KAAA,CAAM5H;YAAM,CAAC;YACxD;UACF;UAEA,MAAMyE,KAAA,GAAQmD,KAAA,CAAMnD,KAAA;UAGpB,IAAI,WAAWA,KAAA,EAAO;YACpB5E,YAAA,GAAe;YACfkH,UAAA,CAAWC,OAAA,CAAQ;cAAEvJ,IAAA,EAAM;cAASuC,KAAA,EAAOyE,KAAA,CAAMzE;YAAM,CAAC;YACxD;UACF;UAEA,IAAIwH,YAAA,EAAc;YAChBA,YAAA,GAAe;YAEfT,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN,GAAG6B,mBAAA,CAAoBmF,KAAK;YAC9B,CAAC;UACH;UAEA,IAAIA,KAAA,CAAMwB,KAAA,IAAS,MAAM;YACvBA,KAAA,GAAQ;cACNC,YAAA,EAAczB,KAAA,CAAMwB,KAAA,CAAME,aAAA;cAC1BE,gBAAA,EAAkB5B,KAAA,CAAMwB,KAAA,CAAMK;YAChC;UACF;UAEA,MAAMjB,MAAA,GAASZ,KAAA,CAAMa,OAAA,CAAQ,CAAC;UAE9B,KAAID,MAAA,oBAAAA,MAAA,CAAQW,aAAA,KAAiB,MAAM;YACjCnG,YAAA,GAAeD,+BAAA,CACbyF,MAAA,CAAOW,aACT;UACF;UAEA,KAAIX,MAAA,oBAAAA,MAAA,CAAQ3H,IAAA,KAAQ,MAAM;YACxBqJ,UAAA,CAAWC,OAAA,CAAQ;cACjBvJ,IAAA,EAAM;cACN0J,SAAA,EAAW9B,MAAA,CAAO3H;YACpB,CAAC;UACH;QACF;QAEAgL,MAAM3B,UAAA,EAAY;UAChBA,UAAA,CAAWC,OAAA,CAAQ;YACjBvJ,IAAA,EAAM;YACNoC,YAAA;YACAoG;UACF,CAAC;QACH;MACF,CAAC,CACH;MACAM,OAAA,EAAS;QAAEpB,SAAA;QAAWC;MAAY;MAClCR,WAAA,EAAa;QAAEE,OAAA,EAASN;MAAgB;MACxCvB,QAAA;MACAwD,OAAA,EAAS;QAAElC,IAAA,EAAM3F,IAAA,CAAKC,SAAA,CAAU0F,IAAI;MAAE;IACxC;EACF;AACF;AAIA,IAAMiG,wCAAA,GAA2ClB,EAAA,CAAEvJ,MAAA,CAAO;EACxDzB,EAAA,EAAIgL,EAAA,CAAErJ,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACvBV,OAAA,EAAS8J,EAAA,CAAE/I,MAAA,CAAO,EAAEL,OAAA,CAAQ;EAC5BX,KAAA,EAAO+J,EAAA,CAAErJ,MAAA,CAAO,EAAEC,OAAA,CAAQ;EAC1BoF,OAAA,EAASgE,EAAA,CAAEV,KAAA,CACTU,EAAA,CAAEvJ,MAAA,CAAO;IACPrC,IAAA,EAAM4L,EAAA,CAAErJ,MAAA,CAAO;IACf+F,aAAA,EAAesD,EAAA,CAAErJ,MAAA,CAAO;EAC1B,CAAC,CACH;EACAgG,KAAA,EAAOqD,EAAA,CACJvJ,MAAA,CAAO;IACNoG,aAAA,EAAemD,EAAA,CAAE/I,MAAA,CAAO;IACxB+F,iBAAA,EAAmBgD,EAAA,CAAE/I,MAAA,CAAO;EAC9B,CAAC,EACAL,OAAA,CAAQ;AACb,CAAC;AAID,IAAM+J,2CAAA,GAGJxJ,WAAA,IAEA6I,EAAA,CAAEhJ,KAAA,CAAM,CACNgJ,EAAA,CAAEvJ,MAAA,CAAO;EACPzB,EAAA,EAAIgL,EAAA,CAAErJ,MAAA,CAAO,EAAEC,OAAA,CAAQ;EACvBV,OAAA,EAAS8J,EAAA,CAAE/I,MAAA,CAAO,EAAEL,OAAA,CAAQ;EAC5BX,KAAA,EAAO+J,EAAA,CAAErJ,MAAA,CAAO,EAAEC,OAAA,CAAQ;EAC1BoF,OAAA,EAASgE,EAAA,CAAEV,KAAA,CACTU,EAAA,CAAEvJ,MAAA,CAAO;IACPrC,IAAA,EAAM4L,EAAA,CAAErJ,MAAA,CAAO;IACf+F,aAAA,EAAesD,EAAA,CAAErJ,MAAA,CAAO,EAAEC,OAAA,CAAQ;IAClCoI,KAAA,EAAOgB,EAAA,CAAE/I,MAAA,CAAO;EAClB,CAAC,CACH;EACA0F,KAAA,EAAOqD,EAAA,CACJvJ,MAAA,CAAO;IACNoG,aAAA,EAAemD,EAAA,CAAE/I,MAAA,CAAO;IACxB+F,iBAAA,EAAmBgD,EAAA,CAAE/I,MAAA,CAAO;EAC9B,CAAC,EACAL,OAAA,CAAQ;AACb,CAAC,GACDO,WAAA,CACD;;;AEtXH,SAEEiK,kCAAA,QACK;AACP,SACE1O,cAAA,IAAA2O,eAAA,EACAzO,8BAAA,IAAA0O,+BAAA,EACAzO,yBAAA,IAAA0O,0BAAA,EAEAvO,aAAA,IAAAwO,cAAA,QACK;AACP,SAASvO,CAAA,IAAAwO,EAAA,QAAS;AA4BX,IAAMC,8BAAA,GAAN,MAEP;EAmBEvJ,YACEhC,OAAA,EACAiC,QAAA,EACAC,MAAA,EACA;IAtBF,KAASC,oBAAA,GAAuB;IAuB9B,KAAKnC,OAAA,GAAUA,OAAA;IACf,KAAKiC,QAAA,GAAWA,QAAA;IAChB,KAAKC,MAAA,GAASA,MAAA;EAChB;EApBA,IAAIQ,SAAA,EAAmB;IACrB,OAAO,KAAKR,MAAA,CAAOQ,QAAA;EACrB;EAEA,IAAI8I,qBAAA,EAA+B;IApDrC,IAAApO,EAAA;IAqDI,QAAOA,EAAA,QAAK8E,MAAA,CAAOsJ,oBAAA,KAAZ,OAAApO,EAAA,GAAoC;EAC7C;EAEA,IAAIqO,sBAAA,EAAiC;IAxDvC,IAAArO,EAAA;IAyDI,QAAOA,EAAA,QAAK8E,MAAA,CAAOuJ,qBAAA,KAAZ,OAAArO,EAAA,GAAqC;EAC9C;EAYA,MAAMsO,QAAQ;IACZC,MAAA;IACAtG,OAAA;IACAG;EACF,GAEE;IA5EJ,IAAApI,EAAA;IA6EI,IAAIuO,MAAA,CAAO5N,MAAA,GAAS,KAAKyN,oBAAA,EAAsB;MAC7C,MAAM,IAAIP,kCAAA,CAAmC;QAC3CvI,QAAA,EAAU,KAAKA,QAAA;QACf1C,OAAA,EAAS,KAAKA,OAAA;QACdwL,oBAAA,EAAsB,KAAKA,oBAAA;QAC3BG;MACF,CAAC;IACH;IAEA,MAAM;MAAE5G,eAAA;MAAiBC,KAAA,EAAO+B;IAAS,IAAI,MAAMsE,cAAA,CAAc;MAC/D/M,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAAS6F,eAAA,CAAe,KAAKhJ,MAAA,CAAOmD,OAAA,CAAQ,GAAGA,OAAO;MACtDP,IAAA,EAAM;QACJhF,KAAA,EAAO,KAAKE,OAAA;QACZ4L,KAAA,EAAOD,MAAA;QACPE,eAAA,EAAiB;QACjBC,UAAA,EAAY,KAAK7J,QAAA,CAAS6J,UAAA;QAC1BjI,IAAA,EAAM,KAAK5B,QAAA,CAAS4B;MACtB;MACAtB,qBAAA,EAAuB4I,+BAAA,EACrB/N,EAAA,QAAK8E,MAAA,CAAOE,cAAA,KAAZ,OAAAhF,EAAA,GAA8B2D,qCAChC;MACAuE,yBAAA,EAA2B8F,0BAAA,CACzBW,iCACF;MACAvG,WAAA;MACAC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,OAAO;MACLuG,UAAA,EAAYjF,QAAA,CAAS7F,IAAA,CAAKhD,GAAA,CAAI+N,IAAA,IAAQA,IAAA,CAAKC,SAAS;MACpD1F,KAAA,EAAOO,QAAA,CAASP,KAAA,GACZ;QAAE2F,MAAA,EAAQpF,QAAA,CAASP,KAAA,CAAME;MAAc,IACvC;MACJvB,WAAA,EAAa;QAAEE,OAAA,EAASN;MAAgB;IAC1C;EACF;AACF;AAIA,IAAMgH,iCAAA,GAAoCT,EAAA,CAAEhL,MAAA,CAAO;EACjDY,IAAA,EAAMoK,EAAA,CAAEnC,KAAA,CAAMmC,EAAA,CAAEhL,MAAA,CAAO;IAAE4L,SAAA,EAAWZ,EAAA,CAAEnC,KAAA,CAAMmC,EAAA,CAAExK,MAAA,CAAO,CAAC;EAAE,CAAC,CAAC;EAC1D0F,KAAA,EAAO8E,EAAA,CAAEhL,MAAA,CAAO;IAAEoG,aAAA,EAAe4E,EAAA,CAAExK,MAAA,CAAO;EAAE,CAAC,EAAEL,OAAA,CAAQ;AACzD,CAAC;;;AC3HD,SACElE,cAAA,IAAA6P,eAAA,EACA3P,8BAAA,IAAA4P,+BAAA,EACA3P,yBAAA,IAAA4P,0BAAA,EAEAzP,aAAA,IAAA0P,cAAA,QACK;AACP,SAASzP,CAAA,IAAA0P,EAAA,QAAS;AAmBX,IAAMC,0BAAA,GAAN,MAAyD;EAW9DzK,YACWhC,OAAA,EACQiC,QAAA,EACAC,MAAA,EACjB;IAHS,KAAAlC,OAAA,GAAAA,OAAA;IACQ,KAAAiC,QAAA,GAAAA,QAAA;IACA,KAAAC,MAAA,GAAAA,MAAA;IAbnB,KAASC,oBAAA,GAAuB;EAc7B;EAZH,IAAIuK,iBAAA,EAA2B;IA9BjC,IAAAtP,EAAA;IA+BI,QAAOA,EAAA,QAAK6E,QAAA,CAASyK,gBAAA,KAAd,OAAAtP,EAAA,GAAkC;EAC3C;EAEA,IAAIsF,SAAA,EAAmB;IACrB,OAAO,KAAKR,MAAA,CAAOQ,QAAA;EACrB;EAQA,MAAM2B,WAAW;IACf5G,MAAA;IACAkP,CAAA;IACAC,IAAA;IACAC,WAAA;IACAtJ,IAAA;IACAuJ,eAAA;IACAzH,OAAA;IACAG;EACF,GAEE;IAvDJ,IAAApI,EAAA,EAAAC,EAAA,EAAAkH,EAAA,EAAAC,EAAA,EAAAC,EAAA;IAwDI,MAAMjB,QAAA,GAA2C,EAAC;IAElD,IAAIqJ,WAAA,IAAe,MAAM;MACvBrJ,QAAA,CAAS1F,IAAA,CAAK;QACZE,IAAA,EAAM;QACNyF,OAAA,EAAS;QACTE,OAAA,EACE;MACJ,CAAC;IACH;IAEA,IAAIJ,IAAA,IAAQ,MAAM;MAChBC,QAAA,CAAS1F,IAAA,CAAK;QAAEE,IAAA,EAAM;QAAuByF,OAAA,EAAS;MAAO,CAAC;IAChE;IAEA,MAAMsJ,WAAA,IAAcxI,EAAA,IAAAlH,EAAA,IAAAD,EAAA,QAAK8E,MAAA,CAAO8K,SAAA,KAAZ,gBAAA5P,EAAA,CAAuB2P,WAAA,KAAvB,gBAAA1P,EAAA,CAAA2I,IAAA,CAAA5I,EAAA,aAAAmH,EAAA,GAA0C,mBAAIrE,IAAA,CAAK;IACvE,MAAM;MAAE8E,KAAA,EAAO+B,QAAA;MAAUhC;IAAgB,IAAI,MAAMwH,cAAA,CAAc;MAC/DjO,GAAA,EAAK,KAAK4D,MAAA,CAAO5D,GAAA,CAAI;QACnB8G,IAAA,EAAM;QACNpF,OAAA,EAAS,KAAKA;MAChB,CAAC;MACDqF,OAAA,EAAS+G,eAAA,CAAe,KAAKlK,MAAA,CAAOmD,OAAA,CAAQ,GAAGA,OAAO;MACtDP,IAAA,EAAM;QACJhF,KAAA,EAAO,KAAKE,OAAA;QACZvC,MAAA;QACAkP,CAAA;QACAC,IAAA;QACA,KAAIpI,EAAA,GAAAsI,eAAA,CAAgBG,MAAA,KAAhB,OAAAzI,EAAA,GAA0B,CAAC;QAC/BN,eAAA,EAAiB;QACjB,IAAI,KAAKjC,QAAA,CAAS4B,IAAA,GAAO;UAAEA,IAAA,EAAM,KAAK5B,QAAA,CAAS4B;QAAK,IAAI,CAAC;MAC3D;MACAtB,qBAAA,EAAuB8J,+BAAA,EACrB5H,EAAA,QAAKvC,MAAA,CAAOE,cAAA,KAAZ,OAAAqC,EAAA,GAA8B1D,qCAChC;MACAuE,yBAAA,EAA2BgH,0BAAA,CACzBY,mCACF;MACA1H,WAAA;MACAC,KAAA,EAAO,KAAKvD,MAAA,CAAOuD;IACrB,CAAC;IAED,OAAO;MACL0H,MAAA,EAAQpG,QAAA,CAAS7F,IAAA,CAAKhD,GAAA,CAAI+N,IAAA,IAAQA,IAAA,CAAKmB,QAAQ;MAC/C5J,QAAA;MACAuD,QAAA,EAAU;QACR9G,SAAA,EAAW8M,WAAA;QACX/M,OAAA,EAAS,KAAKA,OAAA;QACdqF,OAAA,EAASN;MACX;IACF;EACF;AACF;AAIA,IAAMmI,mCAAA,GAAsCV,EAAA,CAAElM,MAAA,CAAO;EACnDY,IAAA,EAAMsL,EAAA,CAAErD,KAAA,CAAMqD,EAAA,CAAElM,MAAA,CAAO;IAAE8M,QAAA,EAAUZ,EAAA,CAAEhM,MAAA,CAAO;EAAE,CAAC,CAAC;AAClD,CAAC;;;AC3GD,SAAwB6M,oBAAA,QAA4B;AAsF7C,SAASC,uBAMdhJ,OAAA,EAMA;EACA,MAAMiJ,OAAA,GAAUF,oBAAA,CAAqB/I,OAAA,CAAQiJ,OAAO;EACpD,MAAMC,YAAA,GAAelJ,OAAA,CAAQtF,IAAA;EAS7B,MAAMyO,UAAA,GAAaA,CAAA,MAAO;IACxB,IAAInJ,OAAA,CAAQoJ,MAAA,IAAU;MAAEC,aAAA,EAAe,UAAUrJ,OAAA,CAAQoJ,MAAM;IAAG;IAClE,GAAGpJ,OAAA,CAAQe;EACb;EAEA,MAAMuI,oBAAA,GAAwBC,SAAA,KAA0C;IACtEnL,QAAA,EAAU,GAAG8K,YAAY,IAAIK,SAAS;IACtCvP,GAAA,EAAKA,CAAC;MAAE8G;IAAK,MAAM;MACjB,MAAM9G,GAAA,GAAM,IAAIE,GAAA,CAAI,GAAG+O,OAAO,GAAGnI,IAAI,EAAE;MACvC,IAAId,OAAA,CAAQwJ,WAAA,EAAa;QACvBxP,GAAA,CAAIyP,MAAA,GAAS,IAAIC,eAAA,CAAgB1J,OAAA,CAAQwJ,WAAW,EAAErP,QAAA,CAAS;MACjE;MACA,OAAOH,GAAA,CAAIG,QAAA,CAAS;IACtB;IACA4G,OAAA,EAASoI,UAAA;IACThI,KAAA,EAAOnB,OAAA,CAAQmB;EACjB;EAEA,MAAMwI,mBAAA,GAAsBA,CAC1BjO,OAAA,EACAiC,QAAA,GAAyC,CAAC,MACvCiM,eAAA,CAAgBlO,OAAA,EAASiC,QAAQ;EAEtC,MAAMiM,eAAA,GAAkBA,CACtBlO,OAAA,EACAiC,QAAA,GAAyC,CAAC,MAE1C,IAAIF,iCAAA,CAAkC/B,OAAA,EAASiC,QAAA,EAAU;IACvD,GAAG2L,oBAAA,CAAqB,MAAM;IAC9BnL,2BAAA,EAA6B;EAC/B,CAAC;EAEH,MAAM0L,qBAAA,GAAwBA,CAC5BnO,OAAA,EACAiC,QAAA,GAA+C,CAAC,MAEhD,IAAIsI,uCAAA,CACFvK,OAAA,EACAiC,QAAA,EACA2L,oBAAA,CAAqB,YAAY,CACnC;EAEF,MAAMQ,oBAAA,GAAuBA,CAC3BpO,OAAA,EACAiC,QAAA,GAA8C,CAAC,MAE/C,IAAIsJ,8BAAA,CACFvL,OAAA,EACAiC,QAAA,EACA2L,oBAAA,CAAqB,WAAW,CAClC;EAEF,MAAMS,gBAAA,GAAmBA,CACvBrO,OAAA,EACAiC,QAAA,GAA0C,CAAC,MAE3C,IAAIwK,0BAAA,CACFzM,OAAA,EACAiC,QAAA,EACA2L,oBAAA,CAAqB,OAAO,CAC9B;EAEF,MAAMlL,QAAA,GAAWA,CACf1C,OAAA,EACAiC,QAAA,KACGgM,mBAAA,CAAoBjO,OAAA,EAASiC,QAAQ;EAE1CS,QAAA,CAAS4L,aAAA,GAAgBL,mBAAA;EACzBvL,QAAA,CAAS6L,SAAA,GAAYL,eAAA;EACrBxL,QAAA,CAAS8L,eAAA,GAAkBL,qBAAA;EAC3BzL,QAAA,CAAS+L,kBAAA,GAAqBL,oBAAA;EAC9B1L,QAAA,CAASgM,UAAA,GAAaL,gBAAA;EAEtB,OAAO3L,QAAA;AAMT","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}